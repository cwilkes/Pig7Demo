<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Cascading - User Guide</title><meta content="DocBook XSL Stylesheets V1.74.0" name="generator"></head><body link="#0000ff" alink="#0000ff" bgcolor="white" vlink="#840084" text="black"><div class="book" lang="en"><div class="titlepage"><div><div><h1 class="title"><a name="N2000B"></a>Cascading - User Guide</h1></div><div><div class="authorgroup">
      <div class="author"><h3 class="author">
        <span class="orgname">Concurrent, Inc</span>
      </h3></div>
    </div></div><div><div class="mediaobject"><img src="Cascading%20-%20User%20Guide_files/cascading-logo.png" width="90"></div></div><div><p class="releaseinfo">V 1.1</p></div><div><p class="copyright">Copyright Â© 2007-2010 Concurrent, Inc</p></div><div><p class="pubdate">March, 2010</p></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#N20046">1. Cascading</a></span></dt><dd><dl><dt><span class="section"><a href="#N20050">What is Cascading?</a></span></dt><dt><span class="section"><a href="#N2005F">Who should use Cascading?</a></span></dt><dt><span class="section"><a href="#N2007D">What is Apache Hadoop</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N2009B">2. Diving In</a></span></dt><dt><span class="chapter"><a href="#N20105">3. Data Processing</a></span></dt><dd><dl><dt><span class="section"><a href="#N2010F">Introduction</a></span></dt><dt><span class="section"><a href="#N20121">Pipe Assemblies</a></span></dt><dd><dl><dt><span class="section"><a href="#N201B1">Assembling Pipe Assemblies</a></span></dt><dt><span class="section"><a href="#N20274">Each and
        Every Pipes</a></span></dt><dt><span class="section"><a href="#N2035C">GroupBy and CoGroup Pipes</a></span></dt><dt><span class="section"><a href="#N204BB">Sorting</a></span></dt></dl></dd><dt><span class="section"><a href="#N20543">Source and Sink Taps</a></span></dt><dt><span class="section"><a href="#N2065B">Field
      Algebra</a></span></dt><dt><span class="section"><a href="#N20722">Flows</a></span></dt><dd><dl><dt><span class="section"><a href="#N20736">Creating Flows from Pipe Assemblies</a></span></dt><dt><span class="section"><a href="#N20761">Configuring Flows</a></span></dt><dt><span class="section"><a href="#N207A1">Skipping
        Flows</a></span></dt></dl></dd><dt><span class="section"><a href="#N207F5">Creating Flows from a JobConf</a></span></dt><dt><span class="section"><a href="#N2080A">Cascades</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N2084E">4. Executing Processes</a></span></dt><dd><dl><dt><span class="section"><a href="#N2085C">Introduction</a></span></dt><dt><span class="section"><a href="#N2086A">Building</a></span></dt><dt><span class="section"><a href="#N208DD">Configuring</a></span></dt><dt><span class="section"><a href="#N20900">Executing</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N2093E">5. Using and Developing Operations</a></span></dt><dd><dl><dt><span class="section"><a href="#N20948">Introduction</a></span></dt><dt><span class="section"><a href="#N209D5">Functions</a></span></dt><dt><span class="section"><a href="#N20A39">Filter</a></span></dt><dt><span class="section"><a href="#N20A8E">Aggregator</a></span></dt><dt><span class="section"><a href="#N20B01">Buffer</a></span></dt><dt><span class="section"><a href="#N20B7A">Operation and BaseOperation</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N20BBD">6. Advanced Processing</a></span></dt><dd><dl><dt><span class="section"><a href="#N20BC7">SubAssemblies</a></span></dt><dt><span class="section"><a href="#N20C25">Stream
      Assertions</a></span></dt><dt><span class="section"><a href="#N20C8C">Failure
      Traps</a></span></dt><dt><span class="section"><a href="#N20CD6">Event Handling</a></span></dt><dt><span class="section"><a href="#N20D33">Template
      Taps</a></span></dt><dt><span class="section"><a href="#N20D64">Scripting</a></span></dt><dt><span class="section"><a href="#N20D73">Custom Taps and
      Schemes</a></span></dt><dt><span class="section"><a href="#N20DBC">Custom Types and Serialization</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N20E14">7. Built-In Operations</a></span></dt><dd><dl><dt><span class="section"><a href="#N20E1E">Identity Function</a></span></dt><dt><span class="section"><a href="#N20EAD">Debug
      Function</a></span></dt><dt><span class="section"><a href="#N20EE7">Sample and Limit Functions</a></span></dt><dt><span class="section"><a href="#N20F12">Insert Function</a></span></dt><dt><span class="section"><a href="#N20F21">Text Functions</a></span></dt><dt><span class="section"><a href="#N20F92">Regular Expression Operations</a></span></dt><dt><span class="section"><a href="#N2102B">Java Expression Operations</a></span></dt><dt><span class="section"><a href="#N21082">XML Operations</a></span></dt><dt><span class="section"><a href="#N210EE">Assertions</a></span></dt><dt><span class="section"><a href="#N21206">Logical Filter Operators</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N212A0">8. Best Practices</a></span></dt><dd><dl><dt><span class="section"><a href="#N212AA">Unit Testing</a></span></dt><dt><span class="section"><a href="#N212D7">Flow Granularity</a></span></dt><dt><span class="section"><a href="#N212EF">SubAssemblies, not Factories</a></span></dt><dt><span class="section"><a href="#N21304">Give SubAssemblies Logical Responsibilities</a></span></dt><dt><span class="section"><a href="#N21325">Java Operators in Field Names</a></span></dt><dt><span class="section"><a href="#N21337">Debugging Planner Failures</a></span></dt><dt><span class="section"><a href="#N21355">Optimizing Joins</a></span></dt><dt><span class="section"><a href="#N2135E">Debuging Streams</a></span></dt><dt><span class="section"><a href="#N2136D">Handling Good and Bad Data</a></span></dt><dt><span class="section"><a href="#N21382">Maintaining State in Operations</a></span></dt><dt><span class="section"><a href="#N213BB">Custom Types</a></span></dt><dt><span class="section"><a href="#N213C7">Fields Constants</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N213D8">9. CookBook</a></span></dt><dd><dl><dt><span class="section"><a href="#N213E5">Tuples and Fields</a></span></dt><dt><span class="section"><a href="#N2140B">Stream Shaping</a></span></dt><dt><span class="section"><a href="#N21469">Common Operations</a></span></dt><dt><span class="section"><a href="#N2148F">Stream Ordering</a></span></dt><dt><span class="section"><a href="#N214C3">API Usage</a></span></dt></dl></dd><dt><span class="chapter"><a href="#N214EA">10. How It Works</a></span></dt><dd><dl><dt><span class="section"><a href="#N214F4">MapReduce Job Planner</a></span></dt><dt><span class="section"><a href="#N21530">The Cascade Topological Scheduler</a></span></dt></dl></dd></dl></div><div class="list-of-examples"><p><b>List of Examples</b></p><dl><dt>2.1. <a href="#N200B1">Word Counting</a></dt><dt>3.1. <a href="#N201C5">Chaining Pipes</a></dt><dt>3.2. <a href="#N203A2">Grouping a Tuple Stream</a></dt><dt>3.3. <a href="#N203B6">Merging a Tuple Stream</a></dt><dt>3.4. <a href="#N203C7">Joining a Tuple Stream</a></dt><dt>3.5. <a href="#N203F9">Joining a Tuple Stream with Duplicate Fields</a></dt><dt>3.6. <a href="#N204F0">Secondary Sorting</a></dt><dt>3.7. <a href="#N20510">Reversing Secondary Sort Order</a></dt><dt>3.8. <a href="#N20536">Reverse Order by Time</a></dt><dt>3.9. <a href="#N2058F">Creating a new Tap</a></dt><dt>3.10. <a href="#N20622">Overwriting An Existing Resource</a></dt><dt>3.11. <a href="#N2073B">Creating a new Flow</a></dt><dt>3.12. <a href="#N2074C">Binding Taps in a Flow</a></dt><dt>3.13. <a href="#N2077D">Configuring the Application Jar</a></dt><dt>3.14. <a href="#N20829">Creating a new Cascade</a></dt><dt>4.1. <a href="#N208BA">Sample Ant Build - Properties</a></dt><dt>4.2. <a href="#N208C5">Sample Ant Build - Target</a></dt><dt>4.3. <a href="#N20914">Running a Cascading Application</a></dt><dt>5.1. <a href="#N20A01">Custom Function</a></dt><dt>5.2. <a href="#N20A24">Add Values Function</a></dt><dt>5.3. <a href="#N20A62">Custom Filter</a></dt><dt>5.4. <a href="#N20A7C">String Length Filter</a></dt><dt>5.5. <a href="#N20AC6">Custom Aggregator</a></dt><dt>5.6. <a href="#N20AE9">Add Tuples Aggregator</a></dt><dt>5.7. <a href="#N20B39">Custom Buffer</a></dt><dt>5.8. <a href="#N20B5C">Average Buffer</a></dt><dt>6.1. <a href="#N20BD7">Creating a SubAssembly</a></dt><dt>6.2. <a href="#N20BE5">Using a SubAssembly</a></dt><dt>6.3. <a href="#N20BFC">Creating a Split SubAssembly</a></dt><dt>6.4. <a href="#N20C07">Using a Split SubAssembly</a></dt><dt>6.5. <a href="#N20C5A">Adding Assertions</a></dt><dt>6.6. <a href="#N20C6B">Planning Out Assertions</a></dt><dt>6.7. <a href="#N20CBB">Setting Traps</a></dt><dt>7.1. <a href="#N21290">Combining Filters</a></dt></dl></div>
  

  

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N20046"></a>Chapter&nbsp;1.&nbsp;Cascading</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N20050">What is Cascading?</a></span></dt><dt><span class="section"><a href="#N2005F">Who should use Cascading?</a></span></dt><dt><span class="section"><a href="#N2007D">What is Apache Hadoop</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20050"></a>What is Cascading?</h2></div></div></div>
      

      <p>Cascading is an API for defining, sharing, and executing data
      processing workflows on a distributed data grid or cluster.</p>

      <p>Cascading relies on Apache Hadoop. To use Cascading, Hadoop must
      be installed locally for development and testing, and a Hadoop cluster
      must be deployed for production applications.</p>

      <p>Cascading greatly simplifies the complexities with Hadoop
      application development, job creation, and job scheduling.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2005F"></a>Who should use Cascading?</h2></div></div></div>
      

      <p>Cascading was developed to allow organizations to rapidly develop
      complex data processing applications. These applications come in two
      extremes.</p>

      <p>On one hand, there is too much data for a single computing system
      to manage effectively. Developers have decided to adopt Apache Hadoop as
      the base computing infrastructure, but realize that developing
      reasonably useful applications on Hadoop is not trivial. Cascading eases
      the burden on developers by allowing them to rapidly create, refactor,
      test, and execute complex applications that scale linearly across a
      cluster of computers.</p>

      <p>On the other hand, managers and developers realize the complexity
      of the processes in their data canter is getting out of hand with
      one-off data-processing applications living wherever there is enough
      disk space. Subsequently they have decided to adopt Apache Hadoop to
      gain access to its "Global Namespace" file system which allows for a
      single reliable storage framework. Cascading eases the learning curve
      for developers to convert their existing applications for execution on a
      Hadoop cluster. It further allows for developers to create reusable
      libraries and application for use by analysts who need to extract data
      from the Hadoop file system.</p>

      <p>Cascading was designed to support three user roles. The
      application Executor, process Assembler, and the operation
      Developer.</p>

      <p>The application Executor is someone, a developer or analyst, or
      some system (like a cron job) which runs a data processing application
      on a given cluster. This is typically done via the command line using a
      pre-packaged Java Jar file compiled against the Apache Hadoop and
      Cascading libraries. This application may accept parameters to customize
      it for an given execution and generally results in a set of data the
      user will export from the Hadoop file system for some specific
      purpose.</p>

      <p>The process Assembler is someone who assembles data processing
      workflows into unique applications. This is generally a development task
      of chaining together operations that act on input data sets to produce
      one or more output data sets. This task can be done using the raw Java
      Cascading API or via a scripting language like Groovy, JRuby, or
      Jython.</p>

      <p>The operation Developer is someone who writes individual functions
      or operations, typically in Java, or reusable sub-assemblies that act on
      the data that pass through the data processing workflow. A simple
      example would be a parser that takes a string and converts it to an
      Integer. Operations are equivalent to Java functions in the sense that
      they take input arguments and return data. And they can execute at any
      granularity, simply parsing a string, or performing some complex routine
      on the argument data using third-party libraries.</p>

      <p>All three roles can be a developer, but the API allows for a clean
      separation of responsibilities for larger organizations that need
      non-developers to run ad-hoc applications or build production processes
      on a Hadoop cluster.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2007D"></a>What is Apache Hadoop</h2></div></div></div>
      

      <p>From the Hadoop website, it â<span class="quote">is a software platform that
      lets one easily write and run applications that process vast amounts of
      data</span>â.</p>

      <p>To be a little more specific, Hadoop provides a storage layer that
      holds vast amounts of data, and an execution layer for running an
      application in parallel across the cluster against parts of the stored
      data.</p>

      <p>The storage layer, the Hadoop File System (HDFS), looks like a
      single storage volume that has been optimized for many concurrent
      serialized reads of large data files. Where "large" ranges from
      Gigabytes to Petabytes. But it only supports a single writer. Thus
      random access to the data is not really possible in an efficient manner.
      But this is why it is so performant and reliable. Reliable in part
      because this restriction allows for the data to be replicated across the
      cluster reducing the chance of data loss.</p>

      <p>The execution layer relies on a "divide and conquer" strategy
      called MapReduce. MapReduce is beyond the scope of this document, but
      suffice it to say, it can be so difficult to develop "real world"
      applications against that Cascading was created to offset the
      complexity.</p>

      <p>Apache Hadoop is an Open Source Apache project and is freely
      available. It can be downloaded from here the Hadoop website, <a class="link" href="http://hadoop.apache.org/core/" target="_top">http://hadoop.apache.org/core/</a>.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N2009B"></a>Chapter&nbsp;2.&nbsp;Diving In</h2></div></div></div>
    

    <p>Counting words in a document is the most common example presented to
    new Hadoop (and MapReduce) developers, it is the Hadoop equivalent to the
    "Hello World" application.</p>

    <p>Word counting is where a document is parsed into individual words,
    and the frequency of those words are counted.</p>

    <p>For example, if we counted the last paragraph "is" would be counted
    twice, and "document" counted once.</p>

    <p>In the code example below, we will use Cascading to read each line
    of text from a file (our document), parse it into words, then count the
    number of time the word is encountered.</p>

    <div class="example"><a name="N200B1"></a><p class="title"><b>Example&nbsp;2.1.&nbsp;Word Counting</b></p><div class="example-contents">
      

      <pre class="programlisting">// define source and sink Taps.
Scheme sourceScheme = new TextLine( new Fields( "line" ) );
Tap source = new Hfs( sourceScheme, inputPath );

Scheme sinkScheme = new TextLine( new Fields( "word", "count" ) );
Tap sink = new Hfs( sinkScheme, outputPath, SinkMode.REPLACE );

// the 'head' of the pipe assembly
Pipe assembly = new Pipe( "wordcount" );

// For each input Tuple
// parse out each word into a new Tuple with the field name "word"
// regular expressions are optional in Cascading
String regex = "(?&lt;!\\pL)(?=\\pL)[^ ]*(?&lt;=\\pL)(?!\\pL)";
Function function = new RegexGenerator( new Fields( "word" ), regex );
assembly = new Each( assembly, new Fields( "line" ), function );

// group the Tuple stream by the "word" value
assembly = new GroupBy( assembly, new Fields( "word" ) );

// For every Tuple group
// count the number of occurrences of "word" and store result in
// a field named "count"
Aggregator count = new Count( new Fields( "count" ) );
assembly = new Every( assembly, count );

// initialize app properties, tell Hadoop which jar file to use
Properties properties = new Properties();
FlowConnector.setApplicationJarClass( properties, Main.class );

// plan a new Flow from the assembly using the source and sink Taps
// with the above properties
FlowConnector flowConnector = new FlowConnector( properties );
Flow flow = flowConnector.connect( "word-count", source, sink, assembly );

// execute the flow, block until complete
flow.complete();</pre>
    </div></div><br class="example-break">

    <p>There are a couple things to take away from this example.</p>

    <p>First, the pipe assembly is not coupled to the data (the Tap
    instances) until the last moment before execution. That is, file paths or
    references are not embedded in the pipe assembly. The pipe assembly
    remains independent of <span class="emphasis"><em>which</em></span> data it processes until
    execution. The only dependency is <span class="emphasis"><em>what</em></span> the data looks
    like, its "scheme", or the field names that make it up.</p>

    <p>That brings up fields. Every input and output file has field names
    associated with it, and every processing element of the pipe assembly
    either expects certain fields, or creates new fields. This allows the
    developer to self document their code, and allows the Cascading planner to
    "fail fast" during planning if a dependency between elements isn't
    satisfied (used a missing or wrong field name).</p>

    <p>It is also important to point out that pipe assemblies are assembled
    through constructor chaining. This may seem odd but is done for two
    reasons. It keeps the code more concise. And it prevents developers from
    creating "cycles" in the resulting pipe assembly. Pipe assemblies are
    Directed Acyclic Graphs (or DAGs). The Cascading planner cannot handle
    processes that feed themselves, that have cycles (not to say there are
    ways around this that are much safer).</p>

    <p>Notice the very first <code class="code">Pipe</code> instance has a name. That
    instance is the "head" of this particular pipe assembly. Pipe assemblies
    can have any number of heads, and any number of tails. This example does
    not name the tail assembly, but for complex assemblies, tails must be
    named for reasons described below.</p>

    <p>Heads and tails of pipe assemblies generally need names, this is how
    sources and sinks are "bound" to them during planning. In our example
    above, there is only one head and one tail, and subsequently only one
    source and one sink, respectively. So naming in this case is optional,
    it's obvious what goes where. Naming is also useful for self documenting
    pipe assemblies, especially where there are splits, joins, and merges in
    the assembly.</p>

    <p>To paraphrase, our example will:</p>

    <div class="itemizedlist"><ul type="disc"><li>
        <p>read each line of text from a file and give it the field name
        "line",</p>
      </li><li>
        <p>parse each "line" into words by the <code class="code">RegexGenerator</code>
        object which in turn returns each word in the field named
        "word",</p>
      </li><li>
        <p>groups on the field named "word" using the <code class="code">GroupBy</code>
        object,</p>
      </li><li>
        <p>then counts the number of elements in each grouping using the
        <code class="code">Count()</code> object and stores this value in the "count"
        field,</p>
      </li><li>
        <p>finally the "word" and "count" fields are written out.</p>
      </li></ul></div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N20105"></a>Chapter&nbsp;3.&nbsp;Data Processing</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N2010F">Introduction</a></span></dt><dt><span class="section"><a href="#N20121">Pipe Assemblies</a></span></dt><dd><dl><dt><span class="section"><a href="#N201B1">Assembling Pipe Assemblies</a></span></dt><dt><span class="section"><a href="#N20274">Each and
        Every Pipes</a></span></dt><dt><span class="section"><a href="#N2035C">GroupBy and CoGroup Pipes</a></span></dt><dt><span class="section"><a href="#N204BB">Sorting</a></span></dt></dl></dd><dt><span class="section"><a href="#N20543">Source and Sink Taps</a></span></dt><dt><span class="section"><a href="#N2065B">Field
      Algebra</a></span></dt><dt><span class="section"><a href="#N20722">Flows</a></span></dt><dd><dl><dt><span class="section"><a href="#N20736">Creating Flows from Pipe Assemblies</a></span></dt><dt><span class="section"><a href="#N20761">Configuring Flows</a></span></dt><dt><span class="section"><a href="#N207A1">Skipping
        Flows</a></span></dt></dl></dd><dt><span class="section"><a href="#N207F5">Creating Flows from a JobConf</a></span></dt><dt><span class="section"><a href="#N2080A">Cascades</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2010F"></a>Introduction</h2></div></div></div>
      

      <p>The Cascading processing model is based on a "pipes and filters"
      metaphor. The developer uses the Cascading API to assemble pipelines
      that split, merge, group, or join streams of data while applying
      operations to each data record or groups of records.</p>

      <p>In Cascading, we call a data record a Tuple, a pipeline a pipe
      assembly, and a series of Tuples passing through a pipe assembly is
      called a tuple stream.</p>

      <p>Pipe assemblies are assembled independently from what data they
      will process. Before a pipe assembly can be executed, it must be bound
      to data sources and data sinks, called Taps. The process of binding pipe
      assemblies to sources and sinks results in a Flow. Flows can be executed
      on a data cluster like Hadoop.</p>

      <p>Finally, many Flows can be grouped together and executed as a
      single process. If one Flow depends on the output of another Flow, it
      will not be executed until all its data dependencies are satisfied. This
      collection of Flows is called a Cascade.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20121"></a>Pipe Assemblies</h2></div></div></div>
      

      <p>Pipe assemblies define what work should be done against a tuple
      stream, where during runtime tuple streams are read from Tap sources and
      are written to Tap sinks. Pipe assemblies may have multiple sources and
      multiple sinks and they can define splits, merges, and joins to
      manipulate how the tuple streams interact.</p>

      <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/pipes.png" align="middle" width="270"></div>

      <p>There are only five Pipe types: Pipe, Each, GroupBy, CoGroup,
      Every, and SubAssembly.</p>

      <div class="variablelist"><dl><dt><span class="term">Pipe</span></dt><dd>
            <p>The <code class="classname">cascading.pipe.Pipe</code> class is used
            to name branches of pipe assemblies. These names are used during
            planning to bind Taps as either sources or sinks (or as traps, an
            advanced topic). It is also the base class for all other pipes
            described below.</p>
          </dd><dt><span class="term">Each</span></dt><dd>
            <p>The <code class="classname">cascading.pipe.Each</code> pipe applies
            a <code class="classname">Function</code> or <code class="classname">Filter</code>
            Operation to each Tuple that passes through it.</p>
          </dd><dt><span class="term">GroupBy</span></dt><dd>
            <p><code class="classname">cascading.pipe.GroupBy</code> manages one
            input Tuple stream and does exactly as it sounds, that is, groups
            the stream on selected fields in the tuple stream.
            <code class="classname">GroupBy</code> also allows for "merging" of two or
            more tuple stream that share the same field names.</p>
          </dd><dt><span class="term">CoGroup</span></dt><dd>
            <p><code class="classname">cascading.pipe.CoGroup</code> allows for
            "joins" on a common set of values, just like a SQL join. The
            output tuple stream of <code class="classname">CoGroup</code> is the
            joined input tuple streams, where a join can be an Inner, Outer,
            Left, or Right join.</p>
          </dd><dt><span class="term">Every</span></dt><dd>
            <p>The <code class="classname">cascading.pipe.Every</code> pipe applies
            an <code class="classname">Aggregator</code> (like count, or sum) or
            <code class="classname">Buffer</code> (a sliding window) Operation to
            every group of Tuples that pass through it.</p>
          </dd><dt><span class="term">SubAssembly</span></dt><dd>
            <p>The <code class="classname">cascading.pipe.SubAssembly</code> pipe
            allows for nesting reusable pipe assemblies into a Pipe class for
            inclusion in a larger pipe assembly. See the section on <a class="xref" href="#subassemblies">SubAssemblies</a>.</p>
          </dd></dl></div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N201B1"></a>Assembling Pipe Assemblies</h3></div></div></div>
        

        <p>Pipe assemblies are created by chaining
        <code class="classname">cascading.pipe.Pipe</code> classes and
        <code class="classname">Pipe</code> subclasses together. Chaining is
        accomplished by passing previous <code class="classname">Pipe</code> instances
        to the constructor of the next <code class="classname">Pipe</code>
        instance.</p>

        <div class="example"><a name="N201C5"></a><p class="title"><b>Example&nbsp;3.1.&nbsp;Chaining Pipes</b></p><div class="example-contents">
          

          <pre class="programlisting">// the "left hand side" assembly head
Pipe lhs = new Pipe( "lhs" );

lhs = new Each( lhs, new SomeFunction() );
lhs = new Each( lhs, new SomeFilter() );

// the "right hand side" assembly head
Pipe rhs = new Pipe( "rhs" );

rhs = new Each( rhs, new SomeFunction() );

// joins the lhs and rhs
Pipe join = new CoGroup( lhs, rhs );

join = new Every( join, new SomeAggregator() );

join = new GroupBy( join );

join = new Every( join, new SomeAggregator() );

// the tail of the assembly
join = new Each( join, new SomeFunction() );</pre>
        </div></div><br class="example-break">

        <p>The above example, if visualized, would look like the diagram
        below.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/simple-pipe-assembly.png" align="middle" width="450"></div>

        <p>Here are some common stream patterns.</p>

        <div class="variablelist"><dl><dt><span class="term">Split</span></dt><dd>
              <p>A split takes a single stream and sends it down one or
              more paths. This is simply achieved by passing a given
              <code class="classname">Pipe</code> instance to two or more subsequent
              <code class="classname">Pipe</code> instances. Note you can use the
              <code class="classname">Pipe</code> class and name the branch (branch
              names are useful for binding <a class="xref" href="#failure-traps">Failure Traps</a>),
              or with a <code class="classname">Each</code> class.</p>
            </dd><dt><span class="term">Merge</span></dt><dd>
              <p>A merge is where two or more streams with the exact same
              Fields (and types) are treated as a single stream. This is
              achieved by passing two or more <code class="classname">Pipe</code>
              instances to a <code class="classname">GroupBy</code>
              <code class="classname">Pipe</code> instance.</p>
            </dd><dt><span class="term">Join</span></dt><dd>
              <p>A join is where two or more streams are connected by one
              or more common values. See the previous diagram for an
              example.</p>
            </dd></dl></div>

        <p>Besides defining the paths tuple streams take through splits,
        merges, grouping, and joining, pipe assemblies also transform and/or
        filter the stored values in each Tuple. This is accomplished by
        applying an Operation to each Tuple or group of Tuples as the tuple
        stream passes through the pipe assembly. To do that, the values in the
        Tuple typically are given field names, in the same fashion columns are
        named in a database so that they may be referenced or selected.</p>

        <div class="variablelist"><dl><dt><span class="term">Operation</span></dt><dd>
              <p>Operations
              (<code class="classname">cascading.operation.Operation</code>) accept an
              input argument Tuple, and output zero or more result Tuples.
              There are a few sub-types of operations defined below. Cascading
              has a number of generic Operations that can be reused, or
              developers can create their own custom Operations.</p>
            </dd><dt><span class="term">Tuple</span></dt><dd>
              <p>In Cascading, we call each record of data a Tuple
              (<code class="classname">cascading.tuple.Tuple</code>), and a series of
              Tuples are a tuple stream. Think of a Tuple as an Array of
              values where each value can be any
              <code class="classname">java.lang.Object</code> Java type (or
              <code class="code">byte[]</code> array). See the section on <a class="xref" href="#custom-types">Custom Types</a> for supporting non-primitive
              values.</p>
            </dd><dt><span class="term">Fields</span></dt><dd>
              <p>Fields (<code class="classname">cascading.tuple.Fields</code>)
              either declare the field names in a Tuple. Or reference values
              in a Tuple as a selector. Fields can either be string names
              ("first_name"), integer positions (<code class="code">-1</code> for the last
              value), or a substitution (<code class="code">Fields.ALL</code> to select all
              values in the Tuple, like an asterisk (<code class="code">*</code>) in SQL,
              see <a class="xref" href="#field-algebra">Field Algebra</a>).</p>
            </dd></dl></div>
      </div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N20274"></a>Each and
        Every Pipes</h3></div></div></div>
        

        <p>The <code class="classname">Each</code> and <code class="classname">Every</code>
        pipe types are the only pipes that can be used to apply Operations to
        the tuple stream.</p>

        <p>The <code class="classname">Each</code> pipe applies an Operation to
        "each" Tuple as it passes through the pipe assembly. The
        <code class="classname">Every</code> pipe applies an Operation to "every"
        group of Tuples as they pass through the pipe assembly, on the tail
        end of a <code class="classname">GroupBy</code> or
        <code class="classname">CoGroup</code> pipe.</p>

        <p></p><pre class="programlisting">new Each( previousPipe, argumentSelector, operation, outputSelector )</pre><p></p>

        <p></p><pre class="programlisting">new Every( previousPipe, argumentSelector, operation, outputSelector )</pre><p></p>

        <p>Both the <code class="classname">Each</code> and
        <code class="classname">Every</code> pipe take a Pipe instance, an argument
        selector, Operation instance, and a output selector on the
        constructor. Where each selector is a Fields instance.</p>

        <p>The <code class="classname">Each</code> pipe may only apply
        <code class="classname">Functions</code> and <code class="classname">Filters</code> to
        the tuple stream as these operations may only operate on one Tuple at
        a time.</p>

        <p>The <code class="classname">Every</code> pipe may only apply
        <code class="classname">Aggregators</code> and <code class="classname">Buffers</code>
        to the tuple stream as these operations may only operate on groups of
        tuples, one grouping at a time.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/pipe-operation-relationship.png" align="middle" width="540"></div>

        <p>The "argument selector" selects values from the input Tuple to
        be passed to the Operation as argument values. Most Operations declare
        result fields, "declared fields" in the diagram. The "output selector"
        selects the output Tuple from an "appended" version of the input Tuple
        and the Operation result Tuple. This new output Tuple becomes the
        input Tuple to the next pipe in the pipe assembly.</p>

        <p>Note that if a <code class="classname">Function</code> or
        <code class="classname">Aggregator</code> emits more than one Tuple, this
        process will be repeated for each result Tuple against the original
        input Tuple, depending on the output selector, input Tuple values
        could be duplicated across each output Tuple.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/each-operation-relationship.png" align="middle" width="540"></div>

        <p>If the argument selector is not given, the whole input Tuple
        (<code class="code">Fields.ALL</code>) is passed to the Operation as argument
        values. If the result selector is not given on an
        <code class="classname">Each</code> pipe, the Operation results are returned
        by default (<code class="code">Fields.RESULTS</code>), replacing the input Tuple
        values in the tuple stream. This really only applies to
        <code class="classname">Functions</code>, as <code class="classname">Filters</code>
        either discard the input Tuple, or return the input Tuple intact.
        There is no opportunity to provide an output selector.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/every-operation-relationship.png" align="middle" width="540"></div>

        <p>For the <code class="classname">Every</code> pipe, the Aggregator
        results are appended to the input Tuple (<code class="code">Fields.ALL</code>) by
        default.</p>

        <p>It is important to note that the <code class="classname">Every</code>
        pipe associates Aggregator results with the current group Tuple. For
        example, if you are grouping on the field "department" and counting
        the number of "names" grouped by that department, the output Fields
        would be ["department","num_employees"]. This is true for both
        <code class="classname">Aggregator</code>, seen above, and
        <code class="classname">Buffer</code>.</p>

        <p>If you were also adding up the salaries associated with each
        "name" in each "department", the output Fields would be
        ["department","num_employees","total_salaries"]. This is only true for
        chains of <code class="classname">Aggregator</code> Operations, you may not
        chain <code class="classname">Buffer</code> Operations.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/buffer-operation-relationship.png" align="middle" width="540"></div>

        <p>For the <code class="classname">Every</code> pipe when used with a
        <code class="classname">Buffer</code> the behavior is slightly different.
        Instead of associating the Buffer results with the current grouing
        Tuple, they are associated with the current values Tuple, just like an
        <code class="classname">Each</code> pipe does with a
        <code class="classname">Function</code>. This might be slightly more
        confusing, but provides much more flexibility.</p>
      </div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N2035C"></a>GroupBy and CoGroup Pipes</h3></div></div></div>
        

        <p>The <code class="classname">GroupBy</code> and
        <code class="classname">CoGroup</code> pipes serve two roles. First, they emit
        sorted grouped tuple streams allowing for Operations to be applied to
        sets of related Tuple instances. Where "sorted" means the tuple groups
        are emitted from the <code class="classname">GroupBy</code> and
        <code class="classname">CoGroup</code> pipes in sort order of the field values
        the groups were grouped on.</p>

        <p>Second, they allow for two streams to be either merged or
        joined. Where merging allows for two or more tuple streams originating
        from different sources to be treated as a single stream. And joining
        allows two or more streams to be "joined" (in the SQL sense) on a
        common key or set of Tuple values in a Tuple.</p>

        <p>It is not required that an <code class="classname">Every</code> follow
        either <code class="classname">GroupBy</code> or CoGroup, an
        <code class="classname">Each</code> may follow immediately after. But an
        <code class="classname">Every</code> many not follow an
        <code class="classname">Each</code>.</p>

        <p>It is important to note, for both <code class="classname">GroupBy</code>
        and <code class="classname">CoGroup</code>, the values being grouped on must
        be the same type. If your application attempts to
        <code class="classname">GroupBy</code> on the field "size", but the value
        alternates between a <code class="classname">String</code> and a
        <code class="classname">Long</code>, Hadoop will fail internally attempting to
        apply a Java <code class="classname">Comparator</code> to them. This also
        holds true for the secondary sorting sort-by fields in
        <code class="classname">GroupBy</code>.</p>

        <p><code class="classname">GroupBy</code> accepts one or more tuple
        streams. If two or more, they must all have the same field names (this
        is also called a merge, see below).</p>

        <div class="example"><a name="N203A2"></a><p class="title"><b>Example&nbsp;3.2.&nbsp;Grouping a Tuple Stream</b></p><div class="example-contents">
          

          <pre class="programlisting">Pipe groupBy = new GroupBy( assembly, new Fields( "group1", "group2" ) );</pre>
        </div></div><br class="example-break">

        <p>The example above simply creates a new tuple stream where Tuples
        with the same values in "group1" and "group2" can be processed as a
        set by an <code class="classname">Aggregator</code> or
        <code class="classname">Buffer</code> Operation. The resulting stream of
        tuples will be sorted by the values in "group1" and "group2".</p>

        <div class="example"><a name="N203B6"></a><p class="title"><b>Example&nbsp;3.3.&nbsp;Merging a Tuple Stream</b></p><div class="example-contents">
          

          <pre class="programlisting">Pipe[] pipes = Pipe.pipes( lhs, rhs );
Pipe merge = new GroupBy( pipes, new Fields( "group1", "group2" ) );</pre>
        </div></div><br class="example-break">

        <p>This example merges two streams ("lhs" and "rhs") into one tuple
        stream and groups the resulting stream on the fields "group1" and
        "group2", in the same fashion as the previous example.</p>

        <p>CoGroup accepts two or more tuple streams and does not require
        any common field names. The grouping fields must be provided for each
        tuple stream.</p>

        <div class="example"><a name="N203C7"></a><p class="title"><b>Example&nbsp;3.4.&nbsp;Joining a Tuple Stream</b></p><div class="example-contents">
          

          <pre class="programlisting">Fields lhsFields = new Fields( "fieldA", "fieldB" );
Fields rhsFields = new Fields( "fieldC", "fieldD" );
Pipe join = new CoGroup( lhs, lhsFields, rhs, rhsFields, new InnerJoin() );</pre>
        </div></div><br class="example-break">

        <p>This example joins two streams ("lhs" and "rhs") on common
        values. Note that common field names are not required here. Actually,
        if there were any common field names, the Cascading planner would
        throw an error as duplicate field names are not allowed.</p>

        <p>This is significant because of the nature of joining
        streams.</p>

        <p>The first stage of joining has to do with identifying field
        names that represent the grouping key for a given stream. The second
        stage is emitting a new Tuple with the joined values, this includes
        the grouping values, and the other values.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/cogrouping-fields-fail.png" align="middle" width="450"></div>

        <p>In the above example, we see what "logically" happens during a
        join. Here we join two streams on the "url" field which happens to be
        common to both streams. The result is simply two Tuple instances with
        the same "url" appended together into a new Tuple. In practice this
        would fail since the result Tuple has duplicate field names. The
        <code class="classname">CoGroup</code> pipe has the
        <em class="parameter"><code>declaredFields</code></em> argument allowing the developer
        to declare new unique field names for the resulting tuple.</p>

        <div class="example"><a name="N203F9"></a><p class="title"><b>Example&nbsp;3.5.&nbsp;Joining a Tuple Stream with Duplicate Fields</b></p><div class="example-contents">
          

          <pre class="programlisting">Fields common = new Fields( "url" );
Fields declared = new Fields( "url1", "word", "wd_count", "url2", "sentence", "snt_count" );
Pipe join = new CoGroup( lhs, common, rhs, common, declared, new InnerJoin() );</pre>
        </div></div><br class="example-break">

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/cogrouping-fields-pass.png" align="middle" width="450"></div>

        <p>Here we see an example of what the developer could have named
        the fields so the planner would not fail.</p>

        <p>It is important to note that Cascading could just magically
        create a new Tuple by removing the duplicate grouping fields names so
        the user isn't left renaming them. In the above example, the duplicate
        "url" columns could be collapsed into one, as they are the same value.
        This is not done because field names are a user convenience, the
        primary mechanism to manipulate Tuples is through positions, not field
        names. So the result of every Pipe (Each, Every, CoGroup, GroupBy)
        needs to be deterministic. This gives Cascading a big performance
        boost, provides a means for sub-assemblies to be built without
        coupling to any "domain level" concepts (like "first_name", or "url),
        and allows for higher level abstractions to be built on-top of
        Cascading simply.</p>

        <p>In the example above, we explicitly set a Joiner class to join
        our data. The reason <code class="classname">CoGroup</code> is named "CoGroup"
        and not "Join" is because joining data is done after all the parallel
        streams are co-grouped by their common keys. The details are not
        terribly important, but note that a "bag" of data for every input
        tuple stream must be created before an join operation can be
        performed. Each bag consists of all the Tuple instances associated
        with a given grouping Tuple.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/cogrouped-values.png" align="middle" width="450"></div>

        <p>Above we see two bags, one for each tuple stream ("lhs" and
        "rhs"). Each Tuple in bag is independent but all Tuples in both bags
        have the same "url" value since we are grouping on "url", from the
        previous example. A Joiner will match up every Tuple on the "lhs" with
        a Tuple on the "rhs". An InnerJoin is the most common. This is where
        each Tuple on the "lhs" is matched with every Tuple on the "rhs". This
        is the default behaviour one would see in SQL when doing a join. If
        one of the bags was empty, no Tuples would be joined. An OuterJoin
        allows for either bag to be empty, and if that is the case, a Tuple
        full of <code class="code">null</code> values would be substituted.</p>

        <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/joins.png" align="middle" width="270"></div>

        <p>Above we see all supported Joiner types.</p>

        <p></p><pre class="programlisting">LHS = [0,a] [1,b] [2,c]
RHS = [0,A] [2,C] [3,D]</pre>Using the above simple data sets, we
        will define each join type where the values are joined on the first
        position, a numeric value. Note when Cascading joins Tuples, the
        resulting Tuple will contain all the incoming values. The duplicate
        common key(s) is not discarded if given. And on outer joins, where
        there is no equivalent key in the alternate stream, <code class="code">null</code>
        values are used as placeholders.<div class="variablelist"><dl><dt><span class="term">InnerJoin</span></dt><dd>
                <p>An Inner join will only return a joined Tuple if neither
                bag has is empty.</p><pre class="programlisting">[0,a,0,A] [2,c,2,C]</pre><p></p>
              </dd><dt><span class="term">OuterJoin</span></dt><dd>
                <p>An Outer join will join if either the left or right bag
                is empty.</p><pre class="programlisting">[0,a,0,A] [1,b,null,null] [2,c,2,C] [null,null,3,D]</pre><p></p>
              </dd><dt><span class="term">LeftJoin</span></dt><dd>
                <p>A Left join can also be stated as a Left Inner and Right
                Outer join, where it is fine if the right bag is empty.</p>

                <pre class="programlisting">[0,a,0,A] [1,b,null,null] [2,c,2,C]</pre>
              </dd><dt><span class="term">RightJoin</span></dt><dd>
                <p>A Right join can also be stated as a Left Outer and
                Right Inner join, where it is fine if the left bag is
                empty.</p><pre class="programlisting">[0,a,0,A] [2,c,2,C] [null,null,3,D]</pre><p></p>
              </dd><dt><span class="term">MixedJoin</span></dt><dd>
                <p>A Mixed join is where 3 or more tuple streams are
                joined, and each pair must be joined differently. See the
                <code class="classname">cascading.pipe.cogroup.MixedJoin</code> class
                for more details.</p>
              </dd><dt><span class="term"><span class="emphasis"><em>Custom</em></span></span></dt><dd>
                <p>A custom join is where the developer subclasses the
                <code class="classname">cascading.pipe.cogroup.Joiner</code>
                class.</p>
              </dd></dl></div><p></p>
      </div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N204BB"></a>Sorting</h3></div></div></div>
        

        <p>By virtue of the Reduce method, in the MapReduce model
        encapsulated by <code class="classname">GroupBy</code> and
        <code class="classname">CoGroup</code>, all groups of Tuples will be locally
        sorted by their grouping values. That is, both the
        <code class="classname">Aggregator</code> and <code class="classname">Buffer</code>
        Operations will receive groups in their natural sort order. But the
        values associated within those groups are not sorted.</p>

        <p>That is, if we sort on 'lastname' with the tuples <code class="code">[john,
        doe]</code> and <code class="code">[jane, doe]</code>, the 'firstname' values will
        arrive in an arbitrary order to the
        <code class="code">Aggregator.aggregate()</code> method.</p>

        <p>In the below example we provide sorting fields to the
        <code class="classname">GroupBy</code> instance. Now <code class="code">value1</code> and
        <code class="code">value2</code> will arrive in their natural sort order (assuming
        <code class="code">value1</code> and <code class="code">value2</code> are
        <code class="classname">java.lang.Comparable</code>).</p>

        <div class="example"><a name="N204F0"></a><p class="title"><b>Example&nbsp;3.6.&nbsp;Secondary Sorting</b></p><div class="example-contents">
          

          <pre class="programlisting">Fields groupFields = new Fields( "group1", "group2" );
Fields sortFields = new Fields( "value1", "value2" );
Pipe groupBy = new GroupBy( assembly, groupFields, sortFields );</pre>
        </div></div><br class="example-break">

        <p>If we didn't care about the order of <code class="code">value2</code>, would
        could have left it out of the <code class="code">sortFields</code>
        <code class="classname">Fields</code> constructor.</p>

        <p>In this example, we reverse the order of <code class="code">value1</code>
        while keeping the natural order of <code class="code">value2</code>.</p>

        <div class="example"><a name="N20510"></a><p class="title"><b>Example&nbsp;3.7.&nbsp;Reversing Secondary Sort Order</b></p><div class="example-contents">
          

          <pre class="programlisting">Fields groupFields = new Fields( "group1", "group2" );
Fields sortFields = new Fields( "value1", "value2" );

sortFields.setComparator( "value1", Collections.reverseOrder() );

Pipe groupBy = new GroupBy( assembly, groupFields, sortFields );</pre>
        </div></div><br class="example-break">

        <p>Whenever there is an implied sort, during grouping or secondary
        sorting, a custom <code class="classname">java.util.Comparator</code> can be
        supplied to the grouping <code class="classname">Fields</code> or secondary
        sort <code class="classname">Fields</code> to influence the sorting through
        the <code class="code">Fields.setComparator()</code> call.</p>

        <p>Creating a custom <code class="classname">Comparator</code> also allows
        for non-<code class="classname">Comparable</code> classes to be sorted and/or
        grouped on.</p>

        <p>Here is a more practical example were we group by the 'day of
        the year', but want to reverse the order of the Tuples within that
        grouping by 'time of day'.</p>

        <div class="example"><a name="N20536"></a><p class="title"><b>Example&nbsp;3.8.&nbsp;Reverse Order by Time</b></p><div class="example-contents">
          

          <pre class="programlisting">Fields groupFields = new Fields( "year", "month", "day" );
Fields sortFields = new Fields( "hour", "minute", "second" );

sortFields.setComparators(
  Collections.reverseOrder(),   // hour
  Collections.reverseOrder(),   // minute
  Collections.reverseOrder() ); // second

Pipe groupBy = new GroupBy( assembly, groupFields, sortFields );</pre>
        </div></div><br class="example-break">
      </div>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20543"></a>Source and Sink Taps</h2></div></div></div>
      

      <p>All input data comes from, and all output data feeds to, a
      <code class="classname">cascading.tap.Tap</code> instance.</p>

      <p>A Tap represents a resource like a data file on the local file
      system, on a Hadoop distributed file system, or even on Amazon S3. Taps
      can be read from, which makes it a "source", or written to, which makes
      it a "sink". Or, more commonly, Taps can act as both sinks and sources
      when shared between Flows.</p>

      <p>All Taps must have a Scheme associated with them. If the Tap is
      about where the data is, and how to get it, the Scheme is about what the
      data is. Cascading provides three Scheme classes, TextLine,
      TextDelimited, and SequenceFile.</p>

      <div class="variablelist"><dl><dt><span class="term">TextLine</span></dt><dd>
            <p>TextLine reads and writes raw text files and returns Tuples
            with two field names by default, "offset" and "line". These values
            are inherited from Hadoop. When written to, all Tuple values are
            converted to Strings and joined with the TAB character
            (\t).</p>
          </dd><dt><span class="term">TextDelimited</span></dt><dd>
            <p>TextDelimited reads and writes character delimited files
            (csv, tsv, etc). When written to, all Tuple values are converted
            to Strings and joined with the given character delimiter. This
            Scheme can optionally handle quoted values with custom quote
            characters. Further, TextDelimited can coerce each value to a
            primitive type.</p>
          </dd><dt><span class="term">SequenceFile</span></dt><dd>
            <p>SequenceFile is based on the Hadoop Sequence file, which is
            a binary format. When written or read from, all Tuple values are
            saved in their native binary form. This is the most efficient file
            format, but being binary, the result files can only be read by
            Hadoop applications.</p>
          </dd></dl></div>

      <p>The fundamental difference behind <code class="classname">TextLine</code>
      and <code class="classname">SequenceFile</code> schemes is that tuples stored in
      the <code class="classname">SequenceFile</code> remain tuples, so when read,
      they do not need to be parsed. So a typical Cascading application will
      read raw text files, and parse each line into a
      <code class="classname">Tuple</code> for processing. The final Tuples are saved
      via the <code class="classname">SequenceFile</code> scheme so future
      applications can just read the file directly into
      <code class="classname">Tuple</code> instances without the parsing step.</p><div class="example"><a name="N2058F"></a><p class="title"><b>Example&nbsp;3.9.&nbsp;Creating a new Tap</b></p><div class="example-contents">
          

          <pre class="programlisting">Tap tap = new Hfs( new TextLine( new Fields( "line" ) ), path );</pre>
        </div></div><br class="example-break"><p></p>

      <p>The above example creates a new Hadoop FileSystem Tap that can
      read/write raw text files. Since only one field name was provided, the
      "offset" field is discarded, resulting in an input tuple stream with
      only "line" values.</p>

      <p>The three most common Tap classes used are, Hfs, Dfs, and Lfs. The
      MultiSourceTap, MultiSinkTap, and TemplateTap are utility Taps.</p>

      <div class="variablelist"><dl><dt><span class="term">Lfs</span></dt><dd>
            <p>The <code class="classname">cascading.tap.Lfs</code> Tap is used to
            reference local files. Local files are files on the same machine
            your Cascading application is started. Even if a remote Hadoop
            cluster is configured, if a Lfs Tap is used as either a source or
            sink in a Flow, Cascading will be forced to run in "local mode"
            and not on the cluster. This is useful when creating applications
            to read local files and import them into the Hadoop distributed
            file system.</p>
          </dd><dt><span class="term">Dfs</span></dt><dd>
            <p>The <code class="classname">cascading.tap.Dfs</code> Tap is used to
            reference files on the Hadoop distributed file system.</p>
          </dd><dt><span class="term">Hfs</span></dt><dd>
            <p>The <code class="classname">cascading.tap.Hfs</code> Tap uses the
            current Hadoop default file system. If Hadoop is configured for
            "local mode" its default file system will be the local file
            system. If configured as a cluster, the default file system is
            likely the Hadoop distributed file system. The Hfs is convenient
            when writing Cascading applications that may or may not be run on
            a cluster. Lhs and Dfs subclass the Hfs Tap.</p>
          </dd><dt><span class="term">MultiSourceTap</span></dt><dd>
            <p>The <code class="classname">cascading.tap.MultiSourceTap</code> is
            used to tie multiple Tap instances into a single Tap for use as an
            input source. The only restriction is that all the Tap instances
            passed to a new MultiSourceTap share the same Scheme classes (not
            necessarily the same Scheme instance).</p>
          </dd><dt><span class="term">MultiSinkTap</span></dt><dd>
            <p>The <code class="classname">cascading.tap.MultiSinkTap</code> is
            used to tie multiple Tap instances into a single Tap for use as an
            output sink. During runtime, for every Tuple output by the pipe
            assembly each child tap to the MultiSinkTap will sink the
            Tuple.</p>
          </dd><dt><span class="term">TemplateTap</span></dt><dd>
            <p>The <code class="classname">cascading.tap.TemplateTap</code> is used
            to sink tuples into directory paths based on the values in the
            Tuple. More can be read below in <a class="xref" href="#template-tap">Template Taps</a>.</p>
          </dd><dt><span class="term">GlobHfs</span></dt><dd>
            <p>The <code class="classname">cascading.tap.GlobHfs</code> Tap accepts
            Hadoop style 'file globbing' expression patterns. This allows for
            multiple paths to be used as a single source, where all paths
            match the given pattern.</p>
          </dd></dl></div>

      <p>Keep in mind Hadoop cannot source data from directories with
      nested sub-directories, and it cannot write to directories that already
      exist. But you can simply point the <code class="classname">Hfs</code>
      <code class="classname">Tap</code> to a directory of data files and they all
      will be used as input, no need to enumate each individual file into a
      <code class="classname">MultiSourceTap</code>.</p>

      <p>To get around existing directories, the Hadoop related Taps allow
      for a <code class="classname">SinkMode</code> value to be set when
      constructed.</p>

      <p></p><div class="example"><a name="N20622"></a><p class="title"><b>Example&nbsp;3.10.&nbsp;Overwriting An Existing Resource</b></p><div class="example-contents">
          

          <pre class="programlisting">Tap tap = new Hfs( new TextLine( new Fields( "line" ) ), path, SinkMode.REPLACE );</pre>
        </div></div><br class="example-break"><p></p>

      <p>Here are all the modes available by the built-in Tap types.</p>

      <div class="variablelist"><dl><dt><span class="term"><code class="classname">SinkMode.KEEP</code></span></dt><dd>
            <p>This is the default behavior. If the resource exists,
            attempting to write to it will fail.</p>
          </dd><dt><span class="term"><code class="classname">SinkMode.REPLACE</code></span></dt><dd>
            <p>This allows Cascading to delete the file immediately after
            the Flow is started.</p>
          </dd><dt><span class="term"><code class="classname">SinkMode.UPDATE</code></span></dt><dd>
            <p>Allows for new Tap types that have the concept of update or
            append. For example, updating records in a database.</p>
          </dd></dl></div>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2065B"></a>Field
      Algebra</h2></div></div></div>
      

      <p>As can be seen above, the <code class="classname">Each</code> and
      <code class="classname">Every</code> <code class="classname">Pipe</code> classes provide
      a means to merge input Tuple values with Operation result Tuple values
      to create a final output Tuple, which are used as the input to the next
      <code class="classname">Pipe</code> instance. This merging is created through a
      type of "field algebra", and can get rather complicated when factoring
      in Fields sets, a kind of wildcard for specifying certain field
      values.</p>

      <p>Fields sets are constant values on the
      <code class="classname">Fields</code> class and can be used in many places the
      <code class="classname">Fields</code> class is expected. They are:</p><div class="variablelist"><dl><dt><span class="term">Fields.ALL</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.ALL</code>
              constant is a "wildcard" that represents all the current
              available fields.</p>
            </dd><dt><span class="term">Fields.RESULTS</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.RESULTS</code>
              constant set is used to represent the field names of the current
              Operations return values. This Fields set may only be used as an
              output selector on a Pipe where it replaces in the input Tuple
              with the Operation result Tuple in the stream.</p>
            </dd><dt><span class="term">Fields.REPLACE</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.REPLACE</code>
              constant is used as an output selector to inline-replace values
              in the incoming Tuple with the results of an Operation. This is
              a convenience Fields set that allows subsequent Operations to
              'step' on the value with a given field name. The current
              Operation must always use the exact same field names, or the
              <code class="classname">ARGS</code> Fields set.</p>
            </dd><dt><span class="term">Fields.SWAP</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.SWAP</code>
              constant is used as an output selector to swap out Operation
              arguments with its results. Neither the argument and result
              field names or size need to be the same. This is useful for when
              the Operation arguments are no longer necessary and the result
              Fields and values should be appended to the remainder of the
              input field names and Tuple.</p>
            </dd><dt><span class="term">Fields.ARGS</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.ARGS</code>
              constant is used to let a given Operation inherit the field
              names of its argument Tuple. This Fields set is a convenience
              and is typically used when the Pipe output selector is
              <code class="classname">RESULTS</code> or
              <code class="classname">REPLACE</code>. It is specifically used by the
              Identity Function when coercing values from Strings to primitive
              types.</p>
            </dd><dt><span class="term">Fields.GROUP</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.GROUP</code>
              constant represents all the fields used as grouping values in a
              previous Group. If there is no previous Group in the pipe
              assembly, the <code class="classname">GROUP</code> represents all the
              current field names.</p>
            </dd><dt><span class="term">Fields.VALUES</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.VALUES</code>
              constant represent all the fields not used as grouping fields in
              a previous Group.</p>
            </dd><dt><span class="term">Fields.UNKNOWN</span></dt><dd>
              <p>The <code class="classname">cascading.tuple.Fields.UNKNOWN</code>
              constant is used when Fields must be declared, but how many and
              their names is unknown. This allows for arbitrarily length
              Tuples from an input source or some Operation. Use this Fields
              set with caution.</p>
            </dd></dl></div><p></p>

      <p>Below is a reference chart showing common ways to merge input and
      result fields for the desired output fields. See the section on <a class="xref" href="#each-every">Each and Every Pipes</a> for details on the different columns and their
      relationships to the <code class="classname">Each</code> and
      <code class="classname">Every</code> Pipes and Functions, Aggregators, and
      Buffers.</p>

      <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/field-algebra.png" align="middle" width="630"></div>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20722"></a>Flows</h2></div></div></div>
      

      <p>When pipe assemblies are bound to source and sink Taps, a
      <code class="classname">Flow</code> is created. Flows are executable in the
      sense that once created they can be "started" and will begin execution
      on a configured Hadoop cluster.</p>

      <p>Think of a Flow as a data processing workflow that reads data from
      sources, processes the data as defined by the pipe assembly, and writes
      data to the sinks. Input source data does not need to exist when the
      Flow is created, but it must exist when the Flow is executed (unless
      executed as part of a Cascade, see <a class="xref" href="#cascades">Cascades</a>).</p>

      <p>The most common pattern is to create a Flow from an existing pipe
      assembly. But there are cases where a MapReduce job has already been
      created and it makes sense to encapsulate it in a Flow class so that it
      may participate in a Cascade and be scheduled with other Flow instances.
      Both patterns are covered here.</p>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N20736"></a>Creating Flows from Pipe Assemblies</h3></div></div></div>
        

        <div class="example"><a name="N2073B"></a><p class="title"><b>Example&nbsp;3.11.&nbsp;Creating a new Flow</b></p><div class="example-contents">
          

          <pre class="programlisting">Flow flow = new FlowConnector().connect( "flow-name", source, sink, pipe );</pre>
        </div></div><br class="example-break">

        <p>To create a Flow, it must be planned though the FlowConnector
        object. The <code class="code">connect()</code> method is used to create new Flow
        instances based on a set of sink Taps, source Taps, and a pipe
        assembly. The example above is quite trivial.</p>

        <div class="example"><a name="N2074C"></a><p class="title"><b>Example&nbsp;3.12.&nbsp;Binding Taps in a Flow</b></p><div class="example-contents">
          

          <pre class="programlisting">// the "left hand side" assembly head
Pipe lhs = new Pipe( "lhs" );

lhs = new Each( lhs, new SomeFunction() );
lhs = new Each( lhs, new SomeFilter() );

// the "right hand side" assembly head
Pipe rhs = new Pipe( "rhs" );

rhs = new Each( rhs, new SomeFunction() );

// joins the lhs and rhs
Pipe join = new CoGroup( lhs, rhs );

join = new Every( join, new SomeAggregator() );

Pipe groupBy = new GroupBy( join );

groupBy = new Every( groupBy, new SomeAggregator() );

// the tail of the assembly
groupBy = new Each( groupBy, new SomeFunction() );

Tap lhsSource = new Hfs( new TextLine(), "lhs.txt" );
Tap rhsSource = new Hfs( new TextLine(), "rhs.txt" );

Tap sink = new Hfs( new TextLine(), "output" );

Map&lt;String, Tap&gt; sources = new HashMap&lt;String, Tap&gt;();

sources.put( "lhs", lhsSource );
sources.put( "rhs", rhsSource );

Flow flow = new FlowConnector().connect( "flow-name", sources, sink, groupBy );</pre>
        </div></div><br class="example-break">

        <p>The example above expands on our previous pipe assembly example
        by creating source and sink Taps and planning a Flow. Note there are
        two branches in the pipe assembly, one named "lhs" and the other
        "rhs". Cascading uses those names to bind the source Taps to the pipe
        assembly. A HashMap of names and taps must be passed to FlowConnector
        in order to bind Taps to branches.</p>

        <p>Since there is only one tail, the "join" pipe, we don't need to
        bind the sink to a branch name. Nor do we need to pass the heads of
        the assembly to the FlowConnector, it can determine the heads of the
        pipe assembly on the fly. When creating more complex Flows with
        multiple heads and tails, all Taps will need to be explicitly named,
        and the proper <code class="code">connect()</code> method will need be
        called.</p>
      </div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N20761"></a>Configuring Flows</h3></div></div></div>
        

        <p>The FlowConnector constructor accepts the
        <code class="classname">java.util.Property</code> object so that default
        Cascading and Hadoop properties can be passed down through the planner
        to the Hadoop runtime. Subsequently any relevant Hadoop
        <code class="code">hadoop-default.xml</code> properties may be added
        (<code class="code">mapred.map.tasks.speculative.execution</code>,
        <code class="code">mapred.reduce.tasks.speculative.execution</code>, or
        <code class="code">mapred.child.java.opts</code> would be very common).</p>

        <p>One property that must be set for production applications is the
        application Jar class or Jar path.</p>

        <div class="example"><a name="N2077D"></a><p class="title"><b>Example&nbsp;3.13.&nbsp;Configuring the Application Jar</b></p><div class="example-contents">
          

          <pre class="programlisting">Properties properties = new Properties();

// pass in the class name of your application
// this will find the parent jar at runtime
FlowConnector.setApplicationJarClass( properties, Main.class );

// or pass in the path to the parent jar
FlowConnector.setApplicationJarPath( properties, pathToJar );

FlowConnector flowConnector = new FlowConnector( properties );</pre>
        </div></div><br class="example-break">

        <p>More information on packaging production applications can be
        found in <a class="xref" href="#executing-processes">Executing Processes</a>.</p>

        <p>Note the pattern of using a static property setter method
        (<code class="classname">cascading.flow.FlowConnector.setApplicationJarPath</code>),
        other classes that can be used to set properties are
        <code class="classname">cascading.flow.MultiMapReducePlanner</code> and
        <code class="classname">cascading.flow.Flow</code>.</p>

        <p>Since the <code class="classname">FlowConnector</code> can be reused,
        any properties passed on the constructor will be handed to all the
        Flows it is used to create. If Flows need to be created with different
        default properties, a new FlowConnector will need to be instantiated
        with those properties.</p>
      </div>

      <div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="N207A1"></a>Skipping
        Flows</h3></div></div></div>
        

        <p>When a Flow participates in a Cascade, the
        <code class="classname">Flow#isSkip()</code> method is consulted before
        calling <code class="classname">Flow#start()</code> on the flow. By default
        <code class="methodname">isSkip()</code> returns true if any of the sinks are
        stale in relation to the Flow sources. Where stale is if they don't
        exist or the resources are older than the sources.</p>

        <p>This behavior is pluggable through the
        <code class="classname">cascading.flow.FlowSkipStrategy</code> interface. A
        new strategy can be set on a <code class="classname">Flow</code> instance
        after its created.</p>

        <div class="variablelist"><dl><dt><span class="term">FlowSkipIfSinkStale</span></dt><dd>
              <p>The
              <code class="classname">cascading.flow.FlowSkipIfSinkStale</code>
              strategy is the default strategy. Sinks are stale if they don't
              exist or the resources are older than the sources. If the
              SinkMode for the sink Tap is REPLACE, then the Tap will be
              treated as stale.</p>
            </dd><dt><span class="term">FlowSkipIfSinkExists</span></dt><dd>
              <p>The
              <code class="classname">cascading.flow.FlowSkipIfSinkExists</code>
              strategy will skip a Flow if the sink Tap exists, regardless of
              age. If the <code class="classname">SinkMode</code> for the sink Tap is
              <code class="code">REPLACE</code>, then the Tap will be treated as
              stale.</p>
            </dd></dl></div>

        <p>Note <code class="classname">Flow#start()</code> and
        <code class="classname">Flow#complete()</code> will not consult the
        <code class="methodname">isSkip()</code> method and subsequently will always
        try to start the Flow if called. It is up to user code to call
        <code class="classname">isSkip()</code> to decide if the current strategy
        suggests the Flow should be skipped.</p>
      </div>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N207F5"></a>Creating Flows from a JobConf</h2></div></div></div>
      

      <p>If a MapReduce job already exists and needs to be managed by a
      Cascade, then the <code class="classname">cascading.flow.MapReduceFlow</code>
      class should be used. After creating a Hadoop
      <code class="classname">JobConf</code> instance, just pass it into the
      <code class="classname">MapReduceFlow</code> constructor. The resulting
      <code class="classname">Flow</code> instance can be used like any other
      Flow.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2080A"></a>Cascades</h2></div></div></div>
      

      <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/cascade.png" align="middle" width="NaN"></div>

      <p>A Cascade allows multiple Flow instances to be executed as a
      single logical unit. If there are dependencies between the Flows, they
      will be executed in the correct order. Further, Cascades act like ant
      build or Unix "make" files. When run, a Cascade will only execute Flows
      that have stale sinks (output data that is older than the input data),
      by default.</p>

      <div class="example"><a name="N20829"></a><p class="title"><b>Example&nbsp;3.14.&nbsp;Creating a new Cascade</b></p><div class="example-contents">
        

        <pre class="programlisting">CascadeConnector connector = new CascadeConnector();
Cascade cascade = connector.connect( flowFirst, flowSecond, flowThird );</pre>
      </div></div><br class="example-break">

      <p>When passing Flows to the CascadeConnector, order is not
      important. The CascadeConnector will automatically determine what the
      dependencies are between the given Flows and create a scheduler that
      will start each flow as its data sources become available. If two or
      more Flow instances have no dependencies, they will be submitted
      together so they can execute in parallel.</p>

      <p>For more information, see the section on <a class="xref" href="#cascade-scheduler">Topological Scheduling</a>.</p>

      <p>If an instance of
      <code class="classname">cascading.flow.FlowSkipStrategy</code> is given to an
      <code class="classname">Cascade</code> instance via the
      <code class="classname">Cascade#setFlowSkipStrategy()</code> method, it will be
      consulted for every Flow instance managed by the Cascade, all skip
      strategies on the Flow instances will be ignored. For more information
      on skip strategies, see <a class="xref" href="#skipping-flows">Skipping Flows</a>.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N2084E"></a>Chapter&nbsp;4.&nbsp;Executing Processes</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N2085C">Introduction</a></span></dt><dt><span class="section"><a href="#N2086A">Building</a></span></dt><dt><span class="section"><a href="#N208DD">Configuring</a></span></dt><dt><span class="section"><a href="#N20900">Executing</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2085C"></a>Introduction</h2></div></div></div>
      

      <p>Cascading requires Hadoop to be installed and correctly
      configured. Apache Hadoop is an Open Source Apache project and is freely
      available. It can be downloaded from the Hadoop website, <a class="link" href="http://hadoop.apache.org/core/" target="_top">http://hadoop.apache.org/core/</a>.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2086A"></a>Building</h2></div></div></div>
      

      <p>Cascading ships with a handful of jars.</p>

      <div class="variablelist"><dl><dt><span class="term">cascading-1.1.x.jar</span></dt><dd>
            <p>all relevant Cascading class files and libraries, with a
            <code class="filename">lib</code> folder containing all third-party
            dependencies</p>
          </dd><dt><span class="term">cascading-core-1.1.x.jar</span></dt><dd>
            <p>all Cascading Core class files, should be packaged with
            <code class="filename">lib/*.jar</code></p>
          </dd><dt><span class="term">cascading-xml-1.1.x.jar</span></dt><dd>
            <p>all Cascading XML module class files, should be packaged
            with <code class="filename">lib/xml/*.jar</code></p>
          </dd><dt><span class="term">cascading-test-1.1.x.jar</span></dt><dd>
            <p>all Cascading unit tests. If writing custom modules for
            cascading, sub-classing
            <code class="classname">cascading.CascadingTestCase</code> might be
            helpful</p>
          </dd></dl></div>

      <p>Cascading will run with Hadoop in its default 'local' or 'stand
      alone' mode, or configured as a distributed cluster.</p>

      <p>When used on a cluster, a Hadoop job Jar must be created with
      Cascading jars and dependent thrid-party jars in the job jar
      <code class="filename">lib</code> directory, per the Hadoop documentation.</p>

      <div class="example"><a name="N208BA"></a><p class="title"><b>Example&nbsp;4.1.&nbsp;Sample Ant Build - Properties</b></p><div class="example-contents">
        

        <pre class="programlisting">&lt;!-- Common ant build properties, included here for completeness --&gt;
&lt;property name="build.dir" location="${basedir}/build"/&gt;
&lt;property name="build.classes" location="${build.dir}/classes"/&gt;

&lt;!-- Cascading specific properties --&gt;
&lt;property name="cascading.home" location="${basedir}/../cascading"/&gt;
&lt;property file="${cascading.home}/version.properties"/&gt;
&lt;property name="cascading.release.version" value="x.y.z"/&gt;
&lt;property name="cascading.filename.core"
          value="cascading-core-${cascading.release.version}.jar"/&gt;
&lt;property name="cascading.filename.xml"
          value="cascading-xml-${cascading.release.version}.jar"/&gt;
&lt;property name="cascading.libs" value="${cascading.home}/lib"/&gt;
&lt;property name="cascading.libs.core" value="${cascading.libs}"/&gt;
&lt;property name="cascading.libs.xml" value="${cascading.libs}/xml"/&gt;

&lt;condition property="cascading.path" value="${cascading.home}/"
           else="${cascading.home}/build"&gt;
  &lt;available file="${cascading.home}/${cascading.filename.core}"/&gt;
&lt;/condition&gt;

&lt;property name="cascading.lib.core"
          value="${cascading.path}/${cascading.filename.core}"/&gt;
&lt;property name="cascading.lib.xml"
          value="${cascading.path}/${cascading.filename.xml}"/&gt;</pre>
      </div></div><br class="example-break">

      <div class="example"><a name="N208C5"></a><p class="title"><b>Example&nbsp;4.2.&nbsp;Sample Ant Build - Target</b></p><div class="example-contents">
        

        <pre class="programlisting">&lt;!--
  A sample target to jar project classes and Cascading
  libraries into a single Hadoop compatible jar file.
 --&gt;

&lt;target name="jar" description="creates a Hadoop ready jar w/dependencies"&gt;

  &lt;!-- copy Cascading classes and libraries --&gt;
  &lt;copy todir="${build.classes}/lib" file="${cascading.lib.core}"/&gt;
  &lt;copy todir="${build.classes}/lib" file="${cascading.lib.xml}"/&gt;
  &lt;copy todir="${build.classes}/lib"&gt;
    &lt;fileset dir="${cascading.libs.core}" includes="*.jar"/&gt;
    &lt;fileset dir="${cascading.libs.xml}" includes="*.jar"/&gt;
  &lt;/copy&gt;

  &lt;jar jarfile="${build.dir}/${ant.project.name}.jar"&gt;
    &lt;fileset dir="${build.classes}"/&gt;
    &lt;fileset dir="${basedir}" includes="lib/"/&gt;
    &lt;manifest&gt;
      &lt;!-- the project Main class, by default assumes Main --&gt;
      &lt;attribute name="Main-Class" value="${ant.project.name}/Main"/&gt;
    &lt;/manifest&gt;
  &lt;/jar&gt;

&lt;/target&gt;</pre>
      </div></div><br class="example-break">

      <p>The above Ant snippets can be used in your project to create a
      Hadoop jar for submission on your cluster. Again, all Hadoop
      applications that are intended to be run in a cluster must be packaged
      with all third-party libraries in a directory named
      <code class="filename">lib</code> in the final application Jar file, regardless
      if they are Cascading applications or raw Hadoop MapReduce
      applications.</p>

      <p>Note, the snippets above is only intended to show how to include
      Cascading libraries, you still need to compile your project into the
      <span class="property">build.classes</span> path.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N208DD"></a>Configuring</h2></div></div></div>
      

      <p>During runtime, Hadoop must be "told" which application jar file
      should be pushed to the cluster. Typically this is done via the Hadoop
      API JobConf object.</p>

      <p>Cascading offers a shorthand for configuring this
      parameter.</p>

      <pre class="programlisting">Properties properties = new Properties();

// pass in the class name of your application
// this will find the parent jar at runtime
FlowConnector.setApplicationJarClass( properties, Main.class );

// or pass in the path to the parent jar
FlowConnector.setApplicationJarPath( properties, pathToJar );

FlowConnector flowConnector = new FlowConnector( properties );</pre>

      <p>Above we see how to set the same property two ways. First via the
      <code class="methodname">setApplicationJarClass()</code> method, and via the
      <code class="methodname">setApplicationJarPath()</code> method. The first
      method takes a Class object that owns the 'main' function for this
      application. The assumption here is that <code class="code">Main.class</code> is not
      located in a Java Jar that is stored in the <code class="filename">lib</code>
      folder of the application Jar. If it is, that Jar will be pushed to the
      cluster, not the parent application jar.</p>

      <p>In your application, only one of these methods needs to be called,
      but one of them must be called to properly configure Hadoop.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20900"></a>Executing</h2></div></div></div>
      

      <p>Running a Cascading application is exactly the same as running any
      Hadoop application. After packaging your application into a single jar
      (see <a class="xref" href="#building-processes">Building Cascading Applications</a>), you must use
      <code class="filename">bin/hadoop</code> to submit the application to the
      cluster.</p>

      <p>For example, to execute an application stuffed into
      <code class="filename">your-application.jar</code>, call the Hadoop shell
      script:</p>

      <div class="example"><a name="N20914"></a><p class="title"><b>Example&nbsp;4.3.&nbsp;Running a Cascading Application</b></p><div class="example-contents">
        

        <p></p><pre class="programlisting">$HADOOP_HOME/bin/hadoop jar your-application.jar [some params]</pre><p></p>
      </div></div><br class="example-break">

      <p>If the configuration scripts in <code class="envar">$HADOOP_CONF_DIR</code>
      are configured to use a cluster, the Jar will be pushed into the cluster
      for execution.</p>

      <p>Cascading does not rely on any environment variables like
      <code class="envar">$HADOOP_HOME</code> or <code class="envar">$HADOOP_CONF_DIR</code>, only
      <code class="filename">bin/hadoop</code> does.</p>

      <p>It should be noted that even though
      <code class="filename">your-application.jar</code> is passed on the command line
      to <code class="filename">bin/hadoop</code> this in no way configures Hadoop to
      push this jar into the cluster. You must still call one of the property
      setters mentioned above to set the proper path to the application jar.
      If misconfigured, likely one of the internal libraries (found in the lib
      folder) will be pushed to the cluster instead and
      <code class="classname">ClassNotFoundException</code>s will be thrown.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N2093E"></a>Chapter&nbsp;5.&nbsp;Using and Developing Operations</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N20948">Introduction</a></span></dt><dt><span class="section"><a href="#N209D5">Functions</a></span></dt><dt><span class="section"><a href="#N20A39">Filter</a></span></dt><dt><span class="section"><a href="#N20A8E">Aggregator</a></span></dt><dt><span class="section"><a href="#N20B01">Buffer</a></span></dt><dt><span class="section"><a href="#N20B7A">Operation and BaseOperation</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20948"></a>Introduction</h2></div></div></div>
      

      <p>To use Cascading, it is not strictly necessary to create custom
      Operations. There are a number of Operations in the Cascading library
      that can be combined into very robust applications. In the same way you
      can chain <span class="command"><strong>sed</strong></span>, <span class="command"><strong>grep</strong></span>,
      <span class="command"><strong>sort</strong></span>, <span class="command"><strong>uniq</strong></span>,
      <span class="command"><strong>awk</strong></span>, etc in Unix, you can chain existing Cascading
      operations. But developing customs Operations is very simple in
      Cascading.</p>

      <p>There are four kinds of Operations:
      <code class="classname">Function</code>, <code class="classname">Filter</code>,
      <code class="classname">Aggregator</code>, and
      <code class="classname">Buffer</code>.</p>

      <div class="mediaobject" align="center"><img src="Cascading%20-%20User%20Guide_files/operations.png" align="middle" width="270"></div>

      <p>All Operations operate on an input argument Tuple and all
      Operations other than <code class="classname">Filter</code> may return zero or
      more Tuple object results. That is, a <code class="classname">Function</code>
      can parse a string and return a new Tuple for every value parsed out
      (one Tuple for each 'word'), or it may create a single Tuple with every
      parsed value as an element in the Tuple object (one Tuple with
      "first-name" and "last-name" fields).</p>

      <p>In practice, a <code class="classname">Function</code> that returns no
      results is a <code class="classname">Filter</code>, but the
      <code class="classname">Filter</code> type has been optimized and can be
      combined with "logical" filter Operations like
      <code class="classname">Not</code>, <code class="classname">And</code>,
      <code class="classname">Or</code>, etc.</p>

      <p>During runtime, Operations actually receive arguments as an
      instance of the TupleEntry object. The TupleEntry object holds both an
      instance of <code class="classname">Fields</code> and the current
      <code class="classname">Tuple</code> the <code class="classname">Fields</code> object
      defines fields for.</p>

      <p>All Operations, other than <code class="classname">Filter</code>, must
      declare result Fields. For example, if a <code class="classname">Function</code>
      was written to parse words out of a String and return a new Tuple for
      each word, this <code class="classname">Function</code> must declare that it
      intends to return a Tuple with one field named "word". If the
      <code class="classname">Function</code> mistakenly returns more values in the
      Tuple other than a 'word', the process will fail. Operations that do
      return arbitrary numbers of values in a result Tuple may declare
      <code class="code">Fields.UNKNOWN</code>.</p>

      <p>The Cascading planner always attempts to "fail fast" where
      possible by checking the field name dependencies between Pipes and
      Operations, but some cases the planner can't account for.</p>

      <p>All Operations must be wrapped by either an
      <code class="classname">Each</code> or an <code class="classname">Every</code> pipe
      instance. The pipe is responsible for passing in an argument Tuple and
      accepting the result Tuple.</p>

      <p>Operations, by default, are "safe". Safe Operations can execute
      safely multiple times on the same Tuple multiple times, that is, it has
      no side-effects, it is idempotent. If an Operation is not idempotent,
      the method <code class="code">isSafe()</code> must return <code class="code">false</code>. This
      value influences how the Cascading planner renders the Flow under
      certain circumstances.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N209D5"></a>Functions</h2></div></div></div>
      

      <p>A <code class="classname">Function</code> expects a single argument
      <code class="classname">Tuple</code>, and may return zero or more result
      Tuples.</p>

      <p>A <code class="classname">Function</code> may only be used with a
      <code class="classname">Each</code> pipe which may follow any other pipe
      type.</p>

      <p>To create a custom <code class="classname">Function</code>, subclass the
      class <code class="code">cascading.operation.BaseOperation</code> and implement the
      interface<code class="code"> cascading.operation.Function</code>. Because
      <code class="code">BaseOperation</code> has been subclassed, the <code class="code">operate</code>
      method, as defined on the <code class="code">Function</code> interface, is the only
      method that must be implemented.</p>

      <div class="example"><a name="N20A01"></a><p class="title"><b>Example&nbsp;5.1.&nbsp;Custom Function</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SomeFunction extends BaseOperation implements Function
  {
  public void operate( FlowProcess flowProcess, FunctionCall functionCall )
    {
    // get the arguments TupleEntry
    TupleEntry arguments = functionCall.getArguments();

    // create a Tuple to hold our result values
    Tuple result = new Tuple();

    // insert some values into the result Tuple

    // return the result Tuple
    functionCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>Functions should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</p>

      <p>Functions must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number (<code class="code">Operation.ANY</code>) of
      values. Cascading will verify that the number of arguments selected
      match the number of arguments expected during the planning phase.</p>

      <p>Functions may optionally declare the field names they return, by
      default <code class="classname">Functions</code> declare<code class="code">
      Fields.UNKNOWN</code>.</p>

      <p>Both declarations must be done on the constructor, either by
      passing default values to the <code class="code">super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</p>

      <div class="example"><a name="N20A24"></a><p class="title"><b>Example&nbsp;5.2.&nbsp;Add Values Function</b></p><div class="example-contents">
        

        <pre class="programlisting">public class AddValuesFunction extends BaseOperation implements Function
  {
  public AddValuesFunction()
    {
    // expects 2 arguments, fail otherwise
    super( 2, new Fields( "sum" ) );
    }

  public AddValuesFunction( Fields fieldDeclaration )
    {
    // expects 2 arguments, fail otherwise
    super( 2, fieldDeclaration );
    }

  public void operate( FlowProcess flowProcess, FunctionCall functionCall )
    {
    // get the arguments TupleEntry
    TupleEntry arguments = functionCall.getArguments();

    // create a Tuple to hold our result values
    Tuple result = new Tuple();

    // sum the two arguments
    int sum = arguments.getInteger( 0 ) + arguments.getInteger( 1 );

    // add the sum value to the result Tuple
    result.add( sum );

    // return the result Tuple
    functionCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>The example above implements a fully functional
      <code class="classname">Function</code> that accepts two values in the argument
      Tuple, adds them together, and returns the result in a new Tuple.</p>

      <p>The first constructor assumes a default field name this function
      will return, but it is a best practice to always give the user the
      option to override the declared field names to prevent any field name
      collisions that would cause the planner to fail.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20A39"></a>Filter</h2></div></div></div>
      

      <p>A <code class="classname">Filter</code> expects a single argument Tuple
      and returns a boolean value stating whether or not the current Tuple in
      the tuple stream should be discarded.</p>

      <p>A <code class="classname">Filter</code> may only be used with a
      <code class="classname">Each</code> pipe, and it may follow any other pipe
      type.</p>

      <p>To create a custom <code class="classname">Filter</code>, subclass the
      class <code class="code">cascading.operation.BaseOperation</code> and implement the
      interface<code class="code">cascading.operation.Filter</code>. Because
      <code class="code">BaseOperation</code> has been subclassed, the
      <code class="code">isRemove</code> method, as defined on the <code class="code">Filter</code>
      interface, is the only method that must be implemented.</p>

      <div class="example"><a name="N20A62"></a><p class="title"><b>Example&nbsp;5.3.&nbsp;Custom Filter</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SomeFilter extends BaseOperation implements Filter
  {
  public boolean isRemove( FlowProcess flowProcess, FilterCall filterCall )
    {
    // get the arguments TupleEntry
    TupleEntry arguments = filterCall.getArguments();

    // initialize the return result
    boolean isRemove = false;

    // test the argument values and set isRemove accordingly

    return isRemove;
    }
  }</pre>
      </div></div><br class="example-break">

      <p>Filters should declare the number of argument values they
      expect.</p>

      <p>Filters must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number (<code class="code"> Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</p>

      <p>The number of arguments declarations must be done on the
      constructor, either by passing a default value to the <code class="code">super</code>
      constructor, or by accepting the value from the user via a constructor
      implementation.</p>

      <div class="example"><a name="N20A7C"></a><p class="title"><b>Example&nbsp;5.4.&nbsp;String Length Filter</b></p><div class="example-contents">
        

        <pre class="programlisting">public class StringLengthFilter extends BaseOperation implements Filter
  {
  public StringLengthFilter()
    {
    // expects 2 arguments, fail otherwise
    super( 2 );
    }

  public boolean isRemove( FlowProcess flowProcess, FilterCall filterCall )
    {
    // get the arguments TupleEntry
    TupleEntry arguments = filterCall.getArguments();

    // filter out the current Tuple if the first argument length is greater
    // than the second argument integer value
    return arguments.getString( 0 ).length() &gt; arguments.getInteger( 1 );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>The example above implements a fully functional
      <code class="classname">Filter</code> that accepts two arguments and filters out
      the current Tuple if the first argument String length is greater than
      the integer value of the second argument.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20A8E"></a>Aggregator</h2></div></div></div>
      

      <p>An <code class="classname">Aggregator</code> expects set of argument
      Tuples in the same grouping, and may return zero or more result
      Tuples.</p>

      <p>An <code class="classname">Aggregator</code> may only be used with an
      <code class="classname">Every</code> pipe, and it may only follow a
      <code class="classname">GroupBy</code>,<code class="classname">CoGroup</code>, or
      another <code class="classname">Every</code> pipe type.</p>

      <p>To create a custom <code class="classname">Aggregator</code>, subclass the
      class <code class="code">cascading.operation.BaseOperation</code> and implement the
      interface<code class="code">cascading.operation.Aggregator</code>. Because
      <code class="code">BaseOperation</code> has been subclassed, the <code class="code">start</code>,
      <code class="code">aggregate</code>, and <code class="code">complete</code> methods, as defined on
      the <code class="code">Aggregator</code> interface, are the only methods that must be
      implemented.</p>

      <div class="example"><a name="N20AC6"></a><p class="title"><b>Example&nbsp;5.5.&nbsp;Custom Aggregator</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SomeAggregator extends BaseOperation&lt;SomeAggregator.Context&gt;
  implements Aggregator&lt;SomeAggregator.Context&gt;
  {
  public static class Context
    {
    Object value;
    }

  public void start( FlowProcess flowProcess,
                     AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    // get the group values for the current grouping
    TupleEntry group = aggregatorCall.getGroup();

    // create a new custom context object
    Context context = new Context();

    // optionally, populate the context object

    // set the context object
    aggregatorCall.setContext( context );
    }

  public void aggregate( FlowProcess flowProcess,
                         AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    // get the current argument values
    TupleEntry arguments = aggregatorCall.getArguments();

    // get the context for this grouping
    Context context = aggregatorCall.getContext();

    // update the context object
    }

  public void complete( FlowProcess flowProcess,
                        AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    Context context = aggregatorCall.getContext();

    // create a Tuple to hold our result values
    Tuple result = new Tuple();

    // insert some values into the result Tuple based on the context

    // return the result Tuple
    aggregatorCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>Aggregators should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</p>

      <p>Aggregators must accept 1 or more values in a Tuple as arguments,
      by default they will accept any number ( <code class="code">Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</p>

      <p>Aggregators may optionally declare the field names they return, by
      default <code class="classname">Aggregators</code> declare<code class="code">
      Fields.UNKNOWN</code>.</p>

      <p>Both declarations must be done on the constructor, either by
      passing default values to the <code class="code">super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</p>

      <div class="example"><a name="N20AE9"></a><p class="title"><b>Example&nbsp;5.6.&nbsp;Add Tuples Aggregator</b></p><div class="example-contents">
        

        <pre class="programlisting">public class AddTuplesAggregator
    extends BaseOperation&lt;AddTuplesAggregator.Context&gt;
    implements Aggregator&lt;AddTuplesAggregator.Context&gt;
  {
  public static class Context
    {
    long value = 0;
    }

  public AddTuplesAggregator()
    {
    // expects 1 argument, fail otherwise
    super( 1, new Fields( "sum" ) );
    }

  public AddTuplesAggregator( Fields fieldDeclaration )
    {
    // expects 1 argument, fail otherwise
    super( 1, fieldDeclaration );
    }

  public void start( FlowProcess flowProcess,
                     AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    // set the context object, starting at zero
    aggregatorCall.setContext( new Context() );
    }

  public void aggregate( FlowProcess flowProcess,
                         AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    TupleEntry arguments = aggregatorCall.getArguments();
    Context context = aggregatorCall.getContext();

    // add the current argument value to the current sum
    context.value += arguments.getInteger( 0 );
    }

  public void complete( FlowProcess flowProcess,
                        AggregatorCall&lt;Context&gt; aggregatorCall )
    {
    Context context = aggregatorCall.getContext();

    // create a Tuple to hold our result values
    Tuple result = new Tuple();

    // set the sum
    result.add( context.value );

    // return the result Tuple
    aggregatorCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>The example above implements a fully functional
      <code class="classname">Aggregator</code> that accepts one value in the argument
      Tuple, adds all these argument Tuples in the current grouping, and
      returns the result as a new Tuple.</p>

      <p>The first constructor assumes a default field name this
      <code class="classname">Aggregator</code> will return, but it is a best practice
      to always give the user the option to override the declared field names
      to prevent any field name collisions that would cause the planner to
      fail.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20B01"></a>Buffer</h2></div></div></div>
      

      <p>A <code class="classname">Buffer</code> expects set of argument Tuples in
      the same grouping, and may return zero or more result Tuples.</p>

      <p>The <code class="classname">Buffer</code> is very similar to an
      <code class="classname">Aggregator</code> except it receives the current
      Grouping Tuple and an iterator of all the arguments it expects for every
      value Tuple in the current grouping, all on the same method call. This
      is very similar to the typical Reducer interface, and is best used for
      operations that need greater visibility to the previous and next
      elements in the stream. For example, smoothing a series of time-stamps
      where there are missing values.</p>

      <p>An <code class="classname">Buffer</code> may only be used with an
      <code class="classname">Every</code> pipe, and it may only follow a
      <code class="classname">GroupBy</code> or <code class="classname">CoGroup</code> pipe
      type.</p>

      <p>To create a custom <code class="classname">Buffer</code>, subclass the
      class <code class="code">cascading.operation.BaseOperation</code> and implement the
      interface<code class="code">cascading.operation.Buffer</code>. Because
      <code class="code">BaseOperation</code> has been subclassed, the <code class="code">operate</code>
      method, as defined on the <code class="code">Buffer</code> interface, is the only
      method that must be implemented.</p>

      <div class="example"><a name="N20B39"></a><p class="title"><b>Example&nbsp;5.7.&nbsp;Custom Buffer</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SomeBuffer extends BaseOperation implements Buffer
  {
  public void operate( FlowProcess flowProcess, BufferCall bufferCall )
    {
    // get the group values for the current grouping
    TupleEntry group = bufferCall.getGroup();

    // get all the current argument values for this grouping
    Iterator&lt;TupleEntry&gt; arguments = bufferCall.getArgumentsIterator();

    // create a Tuple to hold our result values
    Tuple result = new Tuple();

    while( arguments.hasNext() )
      {
      TupleEntry argument = arguments.next();

      // insert some values into the result Tuple based on the arguemnts
      }

    // return the result Tuple
    bufferCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>Buffer should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</p>

      <p>Buffers must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number ( <code class="code">Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</p>

      <p>Buffers may optionally declare the field names they return, by
      default <code class="classname">Buffers</code> declare
      <code class="code">Fields.UNKNOWN</code>.</p>

      <p>Both declarations must be done on the constructor, either by
      passing default values to the <code class="code">super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</p>

      <div class="example"><a name="N20B5C"></a><p class="title"><b>Example&nbsp;5.8.&nbsp;Average Buffer</b></p><div class="example-contents">
        

        <pre class="programlisting">public class AverageBuffer extends BaseOperation implements Buffer
  {

  public AverageBuffer()
    {
    super( 1, new Fields( "average" ) );
    }

  public AverageBuffer( Fields fieldDeclaration )
    {
    super( 1, fieldDeclaration );
    }

  public void operate( FlowProcess flowProcess, BufferCall bufferCall )
    {
    // init the count and sum
    long count = 0;
    long sum = 0;

    // get all the current argument values for this grouping
    Iterator&lt;TupleEntry&gt; arguments = bufferCall.getArgumentsIterator();

    while( arguments.hasNext() )
      {
      count++;
      sum += arguments.next().getInteger( 0 );
      }

    // create a Tuple to hold our result values
    Tuple result = new Tuple( sum / count );

    // return the result Tuple
    bufferCall.getOutputCollector().add( result );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>The example above implements a fully functional buffer that
      accepts one value in argument Tuple, adds all these argument Tuples in
      the current grouping, and returns the result divided by the number of
      argument tuples counted in a new Tuple.</p>

      <p>The first constructor assumes a default field name this
      <code class="classname">Buffer</code> will return, but it is a best practice to
      always give the user the option to override the declared field names to
      prevent any field name collisions that would cause the planner to
      fail.</p>

      <p>Note this example is somewhat fabricated, in practice a
      <code class="classname">Aggregator</code> should be implemented to compute
      averages. A <code class="classname">Buffer</code> would be better suited for
      "running averages" across very large spans, for example.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20B7A"></a>Operation and BaseOperation</h2></div></div></div>
      

      <p>In all the above sections, the
      <code class="classname">cascading.operation.BaseOperation</code> class was
      subclassed. This class is an implementation of the
      <code class="classname">cascading.operation.Operation</code> interface and
      provides a few default method implementations. It is not strictly
      required to extend <code class="classname">BaseOperation</code>, but it is very
      convenient to do so.</p>

      <p>When developing custom operations, the developer may need to
      initialize and destroy a resource. For example, when doing pattern
      matching, a <code class="classname">java.util.regex.Matcher</code> may need to
      be initialized and used in a thread-safe way. Or a remote connection may
      need to be opened and eventually closed. But for performance reasons,
      the operation should not create/destroy the connection for each Tuple or
      every Tuple group that passes through.</p>

      <p>The interface <code class="interfacename">Operation</code> declares
      two methods, <code class="methodname">prepare()</code> and
      <code class="methodname">cleanup()</code>. In the case of Hadoop and MapReduce,
      the <code class="methodname">prepare()</code> and
      <code class="methodname">cleanup()</code> methods are called once per Map or
      Reduce task. <code class="methodname">prepare()</code> is called before any
      argument Tuple is passed in, and <code class="methodname">cleanup()</code> is
      called after all Tuple arguments have been operated on. Within each of
      these methods, the developer can initialize a "context" object that can
      hold an open socket connection, or <code class="classname">Matcher</code>
      instance. The "context" is user defined and is the same mechanism used
      by the <code class="classname">Aggregator</code> operation, except the
      <code class="classname">Aggregator</code> is also given the opportunity to
      initialize and destroy its context via the
      <code class="classname">start()</code> and <code class="classname">complete()</code>
      methods.</p>

      <p>If a "context" object is used, its type should be declared in the
      sub-class class declaration using the Java Generics notation.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N20BBD"></a>Chapter&nbsp;6.&nbsp;Advanced Processing</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N20BC7">SubAssemblies</a></span></dt><dt><span class="section"><a href="#N20C25">Stream
      Assertions</a></span></dt><dt><span class="section"><a href="#N20C8C">Failure
      Traps</a></span></dt><dt><span class="section"><a href="#N20CD6">Event Handling</a></span></dt><dt><span class="section"><a href="#N20D33">Template
      Taps</a></span></dt><dt><span class="section"><a href="#N20D64">Scripting</a></span></dt><dt><span class="section"><a href="#N20D73">Custom Taps and
      Schemes</a></span></dt><dt><span class="section"><a href="#N20DBC">Custom Types and Serialization</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20BC7"></a>SubAssemblies</h2></div></div></div>
      

      <p>Cascading SubAssemblies are reusable pipe assemblies that are
      linked into larger pipe assemblies. Think of them as subroutines in a
      programming language. The help organize complex pipe assemblies and
      allow for commonly used pipe assemblies to be packaged into libraries
      for inclusion by other users.</p>

      <p>To create a SubAssembly, the
      <code class="classname">cascading.pipe.SubAssembly</code> class must be
      subclassed.</p>

      <div class="example"><a name="N20BD7"></a><p class="title"><b>Example&nbsp;6.1.&nbsp;Creating a SubAssembly</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SomeSubAssembly extends SubAssembly
  {
  public SomeSubAssembly( Pipe lhs, Pipe rhs )
    {
    // continue assembling against lhs
    lhs = new Each( lhs, new SomeFunction() );
    lhs = new Each( lhs, new SomeFilter() );

    // continue assembling against lhs
    rhs = new Each( rhs, new SomeFunction() );

    // joins the lhs and rhs
    Pipe join = new CoGroup( lhs, rhs );

    join = new Every( join, new SomeAggregator() );

    join = new GroupBy( join );

    join = new Every( join, new SomeAggregator() );

    // the tail of the assembly
    join = new Each( join, new SomeFunction() );

    // must register all assembly tails
    setTails( join );
    }
  }</pre>
      </div></div><br class="example-break">

      <p>In the above example, we pass in via the constructor pipes we wish
      to continue assembling against, and the last line we register the 'join'
      pipe as a tail. This allows SubAssemblies to be nested within larger
      pipe assemblies or other SubAssemblies.</p>

      <div class="example"><a name="N20BE5"></a><p class="title"><b>Example&nbsp;6.2.&nbsp;Using a SubAssembly</b></p><div class="example-contents">
        

        <pre class="programlisting">// the "left hand side" assembly head
Pipe lhs = new Pipe( "lhs" );

// the "right hand side" assembly head
Pipe rhs = new Pipe( "rhs" );

// our custom SubAssembly
Pipe pipe = new SomeSubAssembly( lhs, rhs );

pipe = new Each( pipe, new SomeFunction() );</pre>
      </div></div><br class="example-break">

      <p>Above we see how natural it is to include a SubAssembly into a new
      pipe assembly.</p>

      <p>If we had a SubAssembly that represented a split, that is, had two
      or more tails, we could use the <code class="methodname">getTails()</code>
      method to get at the array of "tails" set internally by the
      <code class="methodname">setTails()</code> method.</p>

      <div class="example"><a name="N20BFC"></a><p class="title"><b>Example&nbsp;6.3.&nbsp;Creating a Split SubAssembly</b></p><div class="example-contents">
        

        <pre class="programlisting">public class SplitSubAssembly extends SubAssembly
  {
  public SplitSubAssembly( Pipe pipe )
    {
    // continue assembling against lhs
    pipe = new Each( pipe, new SomeFunction() );

    Pipe lhs = new Pipe( "lhs", pipe );
    lhs = new Each( lhs, new SomeFunction() );

    Pipe rhs = new Pipe( "rhs", pipe );
    rhs = new Each( rhs, new SomeFunction() );

    // must register all assembly tails
    setTails( lhs, rhs );
    }
  }</pre>
      </div></div><br class="example-break">

      <div class="example"><a name="N20C07"></a><p class="title"><b>Example&nbsp;6.4.&nbsp;Using a Split SubAssembly</b></p><div class="example-contents">
        

        <pre class="programlisting">// the "left hand side" assembly head
Pipe head = new Pipe( "head" );

// our custom SubAssembly
SubAssembly pipe = new SplitSubAssembly( head );

// grab the split branches
Pipe lhs = new Each( pipe.getTails()[ 0 ], new SomeFunction() );
Pipe rhs = new Each( pipe.getTails()[ 1 ], new SomeFunction() );</pre>
      </div></div><br class="example-break">

      <p>To rephrase, if a <code class="classname">SubAssembly</code> does not
      split the incoming Tuple stream, the SubAssembly instance can be passed
      directly to the next Pipe instance. But, if the
      <code class="classname">SubAssembly</code> splits the stream into multiple
      branches, each branch tail must be passed to the
      <code class="methodname">setTails()</code> method, and the
      <code class="methodname">getTails()</code> method should be called to get a
      handle to the correct branch to pass to the next
      <code class="classname">Pipe</code> instances.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20C25"></a>Stream
      Assertions</h2></div></div></div>
      

      <p><span class="inlinemediaobject"><img src="Cascading%20-%20User%20Guide_files/stream-assertions.png" alt="A Flow with Stream Assertions" align="middle" width="540"></span></p>

      <p>Stream assertions are simply a mechanism to 'assert' that one or
      more values in a tuple stream meet certain criteria. This is similar to
      the Java language 'assert' keyword, or a unit test. An example would be
      'assert not null' or 'assert matches'.</p>

      <p>Assertions are treated like any other function or aggregator in
      Cascading. They are embedded directly into the pipe assembly by the
      developer. If an assertion fails, the processing stops, by default.
      Alternately they can trigger a Failure Trap.</p>

      <p>As with any test, sometimes they are wanted, and sometimes they
      are unnecessary. Thus stream assertions are embedded as either 'strict'
      or 'validating'.</p>

      <p>When running a tests against regression data, it makes sense to
      use strict assertions. This regression data should be small and
      represent many of the edge cases the processing assembly must support
      robustly. When running tests in staging, or with data that may vary in
      quality since it is from an unmanaged source, using validating
      assertions make much sense. Then there are obvious cases where
      assertions just get in the way and slow down processing and it would be
      nice to just bypass them.</p>

      <p>During runtime, Cascading can be instructed to plan out strict,
      validating, or all assertions before building the final MapReduce jobs
      via the MapReduce Job Planner. And they are truly planned out of the
      resulting job, not just switched off, providing the best
      performance.</p>

      <p>This is just one feature of lazily building MapReduce jobs via a
      planner, instead of hard coding them.</p>

      <div class="example"><a name="N20C5A"></a><p class="title"><b>Example&nbsp;6.5.&nbsp;Adding Assertions</b></p><div class="example-contents">
        

        <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

AssertNotNull notNull = new AssertNotNull();
assembly = new Each( assembly, AssertionLevel.STRICT, notNull );

AssertSizeEquals equals = new AssertSizeEquals( 6 );
assembly = new Each( assembly, AssertionLevel.STRICT, equals );

AssertMatchesAll matchesAll = new AssertMatchesAll( "(GET|HEAD|POST)" );
assembly = new Each( assembly, new Fields("method"),
                     AssertionLevel.STRICT, matchesAll );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>
      </div></div><br class="example-break">

      <p>Again, assertions are added to a pipe assembly like any other
      operation, except the <code class="classname">AssertionLevel</code> must be set,
      so the planner knows how to treat the assertion during planning.</p>

      <div class="example"><a name="N20C6B"></a><p class="title"><b>Example&nbsp;6.6.&nbsp;Planning Out Assertions</b></p><div class="example-contents">
        

        <pre class="programlisting">Properties properties = new Properties();

// removes all assertions from the Flow
FlowConnector.setAssertionLevel( properties, AssertionLevel.NONE );

FlowConnector flowConnector = new FlowConnector( properties );

Flow flow = flowConnector.connect( source, sink, assembly );</pre>
      </div></div><br class="example-break">

      <p>To configure the planner to remove some or all assertions, a
      property must be set via the
      <code class="classname">FlowConnector#setAssertionLevel()</code> method.
      <code class="classname">AssertionLevel.NONE</code> removes all assertions.
      <code class="classname">AssertionLevel.VALID</code> keeps <code class="code">VALID</code>
      assertions but removes <code class="code">STRICT</code> ones. And
      <code class="classname">AssertionLevel.STRICT</code> keeps all assertions, which
      is the planner default value.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20C8C"></a>Failure
      Traps</h2></div></div></div>
      

      <p><span class="inlinemediaobject"><img src="Cascading%20-%20User%20Guide_files/failure-traps.png" alt="A Flow with Failure Traps" align="middle" width="540"></span></p>

      <p>Failure Traps are the same as a Tap sink (opposed to a source),
      except being bound to a particular tail element of the pipe assembly,
      traps can be bound to intermediate pipe assembly segments, like to a
      Stream Assertion.</p>

      <p>Whenever an operation fails and throws an exception, and there is
      an associated trap, the offending Tuple will be saved to the resource
      specified by the trap Tap. This allows the job to continue processing
      without any data loss.</p>

      <p>By design, clusters are hardware fault tolerant. Lose a node, the
      cluster continues working.</p>

      <p>But software fault tolerance is a little different. Failure Traps
      provide a means for the processing to continue without losing track of
      the data that caused the fault. For high fidelity applications, this may
      not be so attractive, but low fidelity applications (like web page
      indexing) this can dramatically improve processing reliability.</p>

      <div class="example"><a name="N20CBB"></a><p class="title"><b>Example&nbsp;6.7.&nbsp;Setting Traps</b></p><div class="example-contents">
        

        <pre class="programlisting">// ...some useful pipes here

// name this pipe assembly segment
assembly = new Pipe( "assertions", assembly );

AssertNotNull notNull = new AssertNotNull();
assembly = new Each( assembly, AssertionLevel.STRICT, notNull );

AssertSizeEquals equals = new AssertSizeEquals( 6 );
assembly = new Each( assembly, AssertionLevel.STRICT, equals );

AssertMatchesAll matchesAll = new AssertMatchesAll( "(GET|HEAD|POST)" );
assembly = new Each( assembly, new Fields("method"),
                     AssertionLevel.STRICT, matchesAll );

// ...some more useful pipes here

Map&lt;String,Tap&gt; traps = new HashMap&lt;String,Tap&gt;();

traps.put( "assertions", trap );

FlowConnector flowConnector = new FlowConnector();
Flow flow =
  flowConnector.connect( "log-parser", source, sink, traps, assembly );</pre>
      </div></div><br class="example-break">

      <p>In the above example, we bind our trap Tap to the pipe assembly
      segment named "assertions". Note how we can name branches and segments
      by using a single <code class="classname">Pipe</code> instance and it applies to
      all subsequent <code class="classname">Pipe</code> instances.</p>

      <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3>
        <p>Traps are for exceptional cases, in the same way Java Exception
        handling is not for application flow control, thus traps are not a
        means to filter some data into other locations. Applications that need
        to filter good and bad data should do so explicitly.</p>
      </div>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20CD6"></a>Event Handling</h2></div></div></div>
      

      <p>Each Flow, has the ability to execute callbacks via an event
      listener. This is very useful when external application need to be
      notified that a Flow has completed.</p>

      <p>A good example is when running Flows on an Amazon EC2 Hadoop
      cluster. After the Flow is completed, a SQS event can be sent notifying
      another application it can now fetch the job results from S3. In tandem,
      it can start the process of shutting down the cluster if no more work is
      queued up for it.</p>

      <p>Flows support event listeners through the
      <code class="classname">cascading.flow.FlowListener</code> interface. The
      FlowListener interface supports four events, <code class="code">onStarting</code>,
      <code class="code">onStopping</code>, <code class="code">onCompleted</code>, and
      <code class="code">onThrowable</code>.</p>

      <div class="variablelist"><dl><dt><span class="term">onStarting</span></dt><dd>
            <p>The onStarting event is fired when a Flow instance receives
            the <code class="code">start()</code> message.</p>
          </dd><dt><span class="term">onStopping</span></dt><dd>
            <p>The onStopping event is fired when a Flow instance receives
            the <code class="code">stop()</code> message.</p>
          </dd><dt><span class="term">onCompleted</span></dt><dd>
            <p>The onCompleted event is fired when a Flow instance has
            completed all work whether if was success or failed. If there was
            a thrown exception, onThrowable will be fired before this
            event.</p>
          </dd><dt><span class="term">onThrowable</span></dt><dd>
            <p>The onThrowable event is fired if any internal job client
            throws a Throwable type. This throwable is passed as an argument
            to the event. onThrowable should return true if the given
            throwable was handled and should not be rethrown from the
            <code class="code">Flow.complete()</code> method.</p>
          </dd></dl></div>

      <p>FlowListeners are useful when external systems must be notified
      when a Flow has completed or failed.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20D33"></a>Template
      Taps</h2></div></div></div>
      

      <p>The <code class="classname">TemplateTap</code> <code class="classname">Tap</code>
      class provides a simple means to break large datasets into smaller sets
      based on values in the dataset. Typically this is called 'binning' the
      data, where each 'bin' of data is named after values shared by the data
      in that bin. For example, organizing log files by month and year.</p>

      <pre class="programlisting">TextDelimited scheme = new TextDelimited( new Fields( "year", "month", "entry" ), "\t" );
Hfs tap = new Hfs( scheme, path );

String template = "%s-%s"; // dirs named "year-month"
Tap months = new TemplateTap( tap, template, SinkMode.REPLACE );</pre>

      <p>In the above example, we construct a parent
      <code class="classname">Hfs</code> <code class="classname">Tap</code> and pass it to the
      constructor of a <code class="classname">TemplateTap</code> instance along with
      a String format 'template'. This format template is populated in the
      order values are declared via the <code class="classname">Scheme</code> class.
      If more complex path formatting is necessary then you may subclass the
      <code class="classname">TemplateTap</code>.</p>

      <p>Note that you can only create sub-directories to bin data into.
      Hadoop must still write 'part' files into each bin directory.</p>

      <p>One last thing to keep in mind is whether or not 'binning' happens
      during the Map or Reduce phase. By doing a
      <code class="classname">GroupBy</code> on the values that will be used to
      populate the template, binning will happen during the Reduce phase and
      likely scale much better if there are a very large number of unique
      grouping keys.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20D64"></a>Scripting</h2></div></div></div>
      

      <p>Cascading was designed with scripting in mind. Since it is just an
      API, any Java compatible scripting language can import and instantiate
      Cascading classes and create pipe assemblies, flows, and execute those
      flows.</p>

      <p>And if the scripting language in question supports Domain Specific
      Language (DSL) creation, the user can create her own DSL to handle
      common idioms.</p>

      <p>See the Cascading website for publicly available scripting
      language bindings.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20D73"></a>Custom Taps and
      Schemes</h2></div></div></div>
      

      <p>Cascading was designed to be easily configured and enhanced by
      developers. Besides allowing for custom Operations, developers can
      provide custom <code class="classname">Tap</code> and
      <code class="classname">Scheme</code> types so applications can connect to
      system external to Hadoop.</p>

      <p>A Tap represents something "physical", like a file or a database
      table. Subsequently Tap implementations are responsible for life cycle
      issues around the resource they represent, like tests for existence, or
      deleting.</p>

      <p>A Scheme represents a format or representation, like a text format
      for a file, or columns in a table. Schemes are responsible for
      converting the Tap managed resources proprietary format to and from a
      <code class="classname">cascading.tuple.Tuple</code> instance.</p>

      <p>Unfortunately creating custom Taps and Schemes can be an involved
      process and requires some knowledge of Hadoop and the Hadoop FileSystem
      API. Most commonly, the <code class="classname">cascading.tap.Hfs</code> class
      can be subclassed if a new file system is to be supported, assuming
      passing a fully qualified URL to the <code class="classname">Hfs</code>
      constructor isn't sufficient (the <code class="classname">Hfs</code> tap will
      look up a file system based on the URL scheme via the Hadoop FileSystem
      API).</p>

      <p>Delegating to the Hadoop FileSystem API is not a strict
      requirement, but the developer will need to implement a Hadoop
      <code class="classname">org.apache.hadoop.mapred.InputFormat</code> and/or
      .<code class="classname">org.apache.hadoop.mapred.OutputFormat</code> so that
      Hadoop knows how to split and handle the incoming/outgoing data. The
      custom <code class="classname">Scheme</code> is responsible for setting
      <code class="classname">InputFormat</code> and
      <code class="classname">OutputFormat</code> on the
      <code class="classname">JobConf</code> via the <code class="methodname">sinkInit</code>
      and <code class="methodname">sourceInit</code> methods.</p>

      <p>For examples on how to implement a custom Tap and Scheme, see the
      <a class="link" href="http://www.cascading.org/1.1/userguide/htmlsingle/???" target="_top">Cascading Modules</a> page for samples.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20DBC"></a>Custom Types and Serialization</h2></div></div></div>
      

      <p>The <code class="classname">Tuple</code> class is a generic container for
      all <code class="classname">java.lang.Object</code> instances (1.0 required all
      objects be of type <code class="classname">java.lang.Comparable</code>).
      Subsequently any primitive value or custom Class can be stored in a
      <code class="classname">Tuple</code> instance, that is, returned by a
      <code class="classname">Function</code>, <code class="classname">Aggregator</code>, or
      <code class="classname">Buffer</code> as a result value.</p>

      <p>But for this to work any Class that isn't a primitive value or a
      Hadoop <code class="classname">Writable</code> type will need to have a
      corresponding Hadoop 'serialization' class registered in the Hadoop
      configuration files for your cluster. Hadoop
      <code class="classname">Writable</code> types work because there is already a
      generic serialization implementation built into Hadoop. See the Hadoop
      documentation for registering a new serialization helper or to create
      <code class="classname">Writable</code> types. Cascading will automatically
      inherit any registered serialization implementations.</p>

      <p>During serialization and deserialization of
      <code class="classname">Tuple</code> instances that contain custom types, the
      Cascading <code class="classname">Tuple</code> serialization framework will need
      to store the class name (as a <code class="classname">String</code>) before
      serializing the custom object. This can be very space inneficient. To
      overcome this, custom types can add the
      <code class="classname">SerializationToken</code> Java annotation to the custom
      type class. The <code class="classname">SerializationToken</code> annotation
      expects two arrays, one of integers named tokens, and one of Class name
      strings. Both arrays must be the same size, and no token can be less
      than 128 (the first 128 values are for internal use).</p>

      <p>During serialization and deserialization, the token values are
      used instead of the <code class="classname">String</code> Class names to reduce
      the amount of storage used.</p>

      <p>Serialization tokens may also be stored in the Hadoop config files
      or set as a property passed to the <code class="classname">FlowConnector</code>,
      with the property name <code class="code">cascading.serialization.tokens</code>. The
      value of this property is a comma separated list of
      <code class="code">token=classname</code> values.</p>

      <p>Note Cascading will natively serialize/deserialize all primitives
      and byte arrays (<code class="code">byte[]</code>). It also uses the token 127 for
      the Hadoop <code class="classname">BytesWritable</code> class.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N20E14"></a>Chapter&nbsp;7.&nbsp;Built-In Operations</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N20E1E">Identity Function</a></span></dt><dt><span class="section"><a href="#N20EAD">Debug
      Function</a></span></dt><dt><span class="section"><a href="#N20EE7">Sample and Limit Functions</a></span></dt><dt><span class="section"><a href="#N20F12">Insert Function</a></span></dt><dt><span class="section"><a href="#N20F21">Text Functions</a></span></dt><dt><span class="section"><a href="#N20F92">Regular Expression Operations</a></span></dt><dt><span class="section"><a href="#N2102B">Java Expression Operations</a></span></dt><dt><span class="section"><a href="#N21082">XML Operations</a></span></dt><dt><span class="section"><a href="#N210EE">Assertions</a></span></dt><dt><span class="section"><a href="#N21206">Logical Filter Operators</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20E1E"></a>Identity Function</h2></div></div></div>
      

      <p>The <code class="classname">cascading.operation.Identify</code> function
      is used to "shape" a tuple stream. Here are some common patterns.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">Discard unused fields</span></dt><dd>
              <p>Here Identity passes its arguments out as results, thanks
              to the <code class="code">Fields.ARGS</code> field declaration.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

Identity identity = new Identity( Fields.ARGS );
pipe = new Each( pipe, new Fields( "ip", "method" ), identity,
                 Fields.RESULTS );

// outgoing -&gt; "ip", "method"</pre>

              <p>In practice the field declaration can be left out as
              <code class="code">Field.ARGS</code> is the default declaration for the
              Identity function. Additionally <code class="code">Fields.RESULTs</code> can
              be left off as it is the default for the
              <code class="classname">Every</code> pipe.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

pipe = new Each( pipe, new Fields( "ip", "method" ), new Identity() );

// outgoing -&gt; "ip", "method"</pre>
            </dd><dt><span class="term">Rename all fields</span></dt><dd>
              <p>Here Identity renames the incoming arguments. Since
              Fields.RESULTS is implied, the incoming Tuple is replaced by the
              arguments selected and given new field names as declared on
              Identity.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "method"

Identity identity = new Identity( new Fields( "address", "request" ) );
pipe = new Each( pipe, new Fields( "ip", "method" ), identity );

// outgoing -&gt; "address", "request"</pre>

              <p>In the above example, if there were more fields than "ip"
              and "method", it would work fine, all the extra fields would be
              discarded. If the same was true for the next example, the
              planner would fail.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "method"

Identity identity = new Identity( new Fields( "address", "request" ) );
pipe = new Each( pipe, Fields.ALL, identity );

// outgoing -&gt; "address", "request"</pre>

              <p>Since <code class="code">Fields.ALL</code> is the default argument
              selector for the <code class="classname">Each</code> pipe, it can be
              left out.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "method"

Identity identity = new Identity( new Fields( "address", "request" ) );
pipe = new Each( pipe, identity );

// outgoing -&gt; "address", "request"</pre>
            </dd><dt><span class="term">Rename a single field</span></dt><dd>
              <p>Here we rename a single field, but return it along with an
              input Tuple field as the result.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

Fields fieldSelector = new Fields( "address", "method" );
Identity identity = new Identity( new Fields( "address" ) );
pipe = new Each( pipe, new Fields( "ip" ), identity, fieldSelector );

// outgoing -&gt; "address", "method"</pre>
            </dd><dt><span class="term">Coerce values to specific primitive types</span></dt><dd>
              <p>Here we replace the Tuple String values "status" and
              "size" with <code class="classname">int</code> and
              <code class="classname">long</code>, respectively.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

Identity identity = new Identity( Integer.TYPE, Long.TYPE );
pipe = new Each( pipe, new Fields( "status", "size" ), identity );

// outgoing -&gt; "status", "size"</pre>

              <p>Or we can replace just the Tuple String value "status"
              with <code class="classname">int</code> while keeping all the other
              values in the output Tuple.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

Identity identity = new Identity( Integer.TYPE );
pipe = new Each( pipe, new Fields( "status" ), identity,
                 Fields.REPLACE );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20EAD"></a>Debug
      Function</h2></div></div></div>
      

      <p>The <code class="classname">cascading.operation.Debug</code> function is a
      utility <code class="classname">Function</code> (actually, its a
      <code class="classname">Filter</code>) that will print the current argument
      Tuple to either <code class="code">stdout</code> or <code class="code">stderr</code>. Used with
      the <code class="classname">DebugLevel</code> enum values
      <code class="classname">NONE</code>, <code class="classname">DEFAULT</code>, or
      <code class="classname">VERBOSE</code>, different debug levels can be embedded
      in a pipe assembly.</p>

      <p>Below we insert a <code class="classname">Debug</code> operation at the
      <code class="classname">VERBOSE</code> level, but configure the planner to
      remove all <code class="classname">Debug</code> operations from the resulting
      <code class="classname">Flow</code>.</p>

      <pre class="programlisting">Pipe assembly = new Pipe( "assembly" );

// ...
assembly = new Each( assembly, DebugLevel.VERBOSE, new Debug() );
// ...

Properties properties = new Properties();

// tell the planner remove all Debug operations
FlowConnector.setDebugLevel( properties, DebugLevel.NONE );
// ...
FlowConnector flowConnector = new FlowConnector( properties );

Flow flow = flowConnector.connect( "debug", source, sink, assembly );</pre>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20EE7"></a>Sample and Limit Functions</h2></div></div></div>
      

      <p>The Sample and Limit functions are used to limit the number of
      Tuples that pass through a pipe assembly.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">Sample</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.filter.Sample</code> filter
              allows a percentage of tuples to pass.</p>
            </dd><dt><span class="term">Limit</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.filter.Limit</code> filter
              allows a set number of Tuples to pass.</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20F12"></a>Insert Function</h2></div></div></div>
      

      <p>The <code class="classname">cascading.operation.Insert</code> function
      allows for insertion of constant literal values into the tuple
      stream.</p>

      <p>This is most useful when a splitting a tuple stream and one of the
      branches needs some identifying value. Or when some missing parameter or
      value, like a date String for the current date, needs to be
      inserted.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20F21"></a>Text Functions</h2></div></div></div>
      

      <p>Cascading includes a number of text functions in the
      <code class="classname">cascading.operation.text</code> package.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">FieldJoiner</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.text.FieldJoiner</code>
              function joins all the values in a Tuple with a given delimiter
              and stuffs the result into a new field.</p>
            </dd><dt><span class="term">FieldFormatter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.text.FieldFormatter</code>
              function formats Tuple values with a given String format and
              stuffs the result into a new field. The
              <code class="classname">java.util.Formatter</code> class is used to
              create a new formatted String.</p>
            </dd><dt><span class="term">DateParser</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.text.DateParser</code>
              function is used to convert a text date String to a timestamp
              using the <code class="classname">java.text.SimpleDateFormat</code>
              syntax. The timestamp is a <code class="classname">long</code> value
              representing the number of milliseconds since January 1, 1970,
              00:00:00 GMT. By default it emits a field with the name "ts" for
              timestamp, but this can be overridden by passing a declared
              Fields value.</p>

              <pre class="programlisting">// "time" -&gt; 01/Sep/2007:00:01:03 +0000

DateParser dateParser = new DateParser( "dd/MMM/yyyy:HH:mm:ss Z" );
pipe = new Each( pipe, new Fields( "time" ), dateParser );

// outgoing -&gt; "ts" -&gt; 1188604863000</pre>

              <p>Above we convert an Apache log style date-time field into
              a <code class="classname">long</code> timestamp.</p>
            </dd><dt><span class="term">DateFormatter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.text.DateFormatter</code>
              function is used to convert a date timestamp to a formatted
              String. This function expects a <code class="classname">long</code>
              value representing the number of milliseconds since January 1,
              1970, 00:00:00 GMT. And uses the
              <code class="classname">java.text.SimpleDateFormat</code> syntax.</p>

              <pre class="programlisting">// "ts" -&gt; 1188604863000

DateFormatter formatter =
  new DateFormatter( new Fields("date"), "dd/MMM/yyyy" );
pipe = new Each( pipe, new Fields( "ts" ), formatter );

// outgoing -&gt; "date" -&gt; 31/Aug/2007</pre>

              <p>Above we convert a <code class="classname">long</code> timestamp
              ("ts") to a date String.</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N20F92"></a>Regular Expression Operations</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">RegexSplitter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexSplitter</code>
              function will split an argument value by a regex pattern String.
              Internally, this function uses
              <code class="classname">java.util.regex.Pattern#split()</code>, thus
              behaves accordingly. By default this function splits on the TAB
              character ("\t"). If a known number of values will emerge from
              this function, it can declare field names. In this case, if the
              splitter encounters more split values than field names, the
              remaining values will be discarded, see
              <code class="classname">java.util.regex.Pattern#split( input, limit
              )</code> for more information.</p>
            </dd><dt><span class="term">RegexParser</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexParser</code>
              function is used to extract a regular expression matched value
              from an incoming argument value. If the regular expression is
              sufficiently complex, and int array may be provided which
              specifies which regex groups should be returned into which field
              names.</p>

              <pre class="programlisting">// incoming -&gt; "line"

String regex =
  "^([^ ]*) +[^ ]* +[^ ]* +\\[([^]]*)\\] +" +
  "\\\"([^ ]*) ([^ ]*) [^ ]*\\\" ([^ ]*) ([^ ]*).*$";
Fields fieldDeclaration =
  new Fields( "ip", "time", "method", "event", "status", "size" );
int[] groups = {1, 2, 3, 4, 5, 6};
RegexParser parser = new RegexParser( fieldDeclaration, regex, groups );
assembly = new Each( assembly, new Fields( "line" ), parser );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>

              <p>Above, we parse an Apache log "line" into its parts. Note
              the int[] groups array starts at 1, not 0. Group 0 is the whole
              group, so if included the first field would be a copy of "line"
              and not "ip".</p>
            </dd><dt><span class="term">RegexReplace</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexReplace</code>
              function is used to replace a regex matched value with a
              replacement value. It maybe used in a "replace all" or "replace
              first" mode. See
              <code class="classname">java.util.regex.Matcher#replaceAll()</code> and
              <code class="classname">java.util.regex.Matcher#replaceFirst()</code>
              methods.</p>

              <pre class="programlisting">// incoming -&gt; "line"

RegexReplace replace =
  new RegexReplace( new Fields( "clean-line" ), "\\s+", " ", true );
assembly = new Each( assembly, new Fields( "line" ), replace );

// outgoing -&gt; "clean-line"</pre>

              <p>Above we replace all adjoined white space characters with
              a single space character.</p>
            </dd><dt><span class="term">RegexFilter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexFilter</code>
              function will apply a regular expression pattern String against
              every input Tuple value and filter the Tuple stream accordingly.
              By default, Tuples that match the given pattern are kept, and
              Tuples that do not match are filtered out. This can be changed
              by setting "removeMatch" to <code class="code">true</code>. Also, by default,
              the whole Tuple is matched against the given pattern String (TAB
              delimited). If "matchEachElement" is set to <code class="code">true</code>,
              the pattern is applied to each Tuple value individually. See the
              <code class="classname">java.util.regex.Matcher#find()</code>
              method.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

Filter filter = new RegexFilter( "^68\\..*" );
assembly = new Each( assembly, new Fields( "ip" ), filter );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>

              <p>Above we keep all lines where the "ip" address starts with
              "68.".</p>
            </dd><dt><span class="term">RegexGenerator</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexGenerator</code>
              function will emit a new Tuple for every matched regular
              expression group, instead of a Tuple with every group as a
              value.</p>

              <pre class="programlisting">// incoming -&gt; "line"

String regex = "(?&lt;!\\pL)(?=\\pL)[^ ]*(?&lt;=\\pL)(?!\\pL)";
Function function = new RegexGenerator( new Fields( "word" ), regex );
assembly = new Each( assembly, new Fields( "line" ), function );

// outgoing -&gt; "word"</pre>

              <p>Above each "line" in a document is parsed into unique
              words and stored in the "word" field of each result
              Tuple.</p>
            </dd><dt><span class="term">RegexSplitGenerator</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.regex.RegexSplitGenerator</code>
              function will emit a new Tuple for every split on the incoming
              argument value delimited by the given pattern String. The
              behavior is similar to the RegexSplitter function.</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2102B"></a>Java Expression Operations</h2></div></div></div>
      

      <p>Cascading provides some support for dynamically compiled Java
      expression to be used as either <code class="classname">Functions</code> or
      <code class="classname">Filters</code>. This functionality is provided by the
      Janino embedded compiler. Janino and its documentation can be found on
      its website, <a class="link" href="http://www.janino.net/" target="_top">http://www.janino.net/</a>. But
      in short, an Expression is a single line of Java, for example <code class="code">a +
      3 * 2</code> or <code class="code">a &lt; 7</code>. The first would resolve to some
      number, the second to a boolean value. Where <code class="code">a</code> and
      <code class="code">b</code> are field names passed in as Tuple arguments to the
      Operation. Janino will compile this expression into byte code giving
      compiled code processing speeds.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">ExpressionFunction</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.expression.ExpressionFunction</code>
              function dynamically resolves a given expression using argument
              Tuple values as inputs to the fields specified in the
              expression.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

String exp =
  "\"this \" + method + \" request was \" + size + \" bytes\"";
Fields fields = new Fields( "pretty" );
ExpressionFunction function =
  new ExpressionFunction( fields, exp, String.class );

assembly =
  new Each( assembly, new Fields( "method", "size" ), function );

// outgoing -&gt; "pretty" = "this GET request was 1282652 bytes"</pre>

              <p>Above, we create a new String value form our expression.
              Note we must declare the type of every input Tuple value so the
              expression compiler knows how to treat the variables in the
              expression.</p>
            </dd><dt><span class="term">ExpressionFilter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.expression.ExpressionFilter</code>
              filter dynamically resolves a given expression using argument
              Tuple values as inputs to the fields specified in the
              expression. Any Tuple that returns true for the given expression
              will be removed from the stream.</p>

              <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

ExpressionFilter filter =
  new ExpressionFilter( "status != 200", Integer.TYPE );

assembly = new Each( assembly, new Fields( "status" ), filter );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>

              <p>Above, every line in the Apache log that does not have a
              "200" status will be filtered out. Notice that the "status"
              would be a String in this example if it was emitted from a
              RegexParser, if so the ExpressionFilter will coerce the value
              from a String to an <code class="classname">int</code> for the
              comparison.</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21082"></a>XML Operations</h2></div></div></div>
      

      <p>All XML Operations are kept in a module other than core, so can be
      included in a Cascading application by including the
      <code class="filename">cascading-xml-x.y.z.jar</code> in the project. This module
      has one dependency, the TagSoup library, which allows for HTML and XML
      "tidying". More about TagSoup can be read on its website, <a class="link" href="http://home.ccil.org/%7Ecowan/XML/tagsoup/" target="_top">http://home.ccil.org/~cowan/XML/tagsoup/</a>.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">XPathParser</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.xml.XPathParser</code>
              function will extract a value from the passed Tuple argument
              into a new Tuple field value. One Tuple value for every given
              XPath expression will be created. This function effectively
              converts an XML document into a table. If the returned value of
              the expression is a <code class="classname">NodeList</code>, only the
              first <code class="classname">Node</code> is used. The
              <code class="classname">Node</code> is converted to a new XML document
              and converted to a String. If only the text values are required,
              search on the <code class="code">text()</code> nodes, or consider using
              XPathGenerator to handle multiple
              <code class="classname">NodeList</code> values.</p>
            </dd><dt><span class="term">XPathGenerator</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.xml.XPathGenerator</code>
              function is a generator function that will emit a new Tuple for
              every <code class="classname">Node</code> returned by the given XPath
              expression.</p>
            </dd><dt><span class="term">XPathFilter</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.xml.XPathFilter</code>
              filter will filter out a Tuple if the given XPath expression
              returns <code class="code">false</code>. Set the removeMatch parameter to
              <code class="code">true</code> if the filter should be reversed.</p>
            </dd><dt><span class="term">TapSoupParser</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.xml.TagSoupParser</code>
              function uses the Tag Soup library to convert incoming HTML to
              clean XHTML. Use the <code class="code">setFeature( feature, value )</code>
              method to set TagSoup specific features (as documented on the
              TagSoup website listed above).</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N210EE"></a>Assertions</h2></div></div></div>
      

      <p>Cascading Stream Assertions are used to build robust reusable pipe
      assemblies. They can be planned out of a Flow instance during runtime.
      For more information see the section on <a class="xref" href="#stream-assertions">Stream Assertions</a>. Below we describe the Assertions
      available in the core library.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">AssertEquals</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertEquals</code>
              Assertion asserts the number of values given on the constructor
              is equal to the number of argument Tuple values and that each
              constructor value is <code class="code">.equals()</code> to its corresponding
              argument value.</p>
            </dd><dt><span class="term">AssertNotEquals</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertNotEquals</code>
              Assertion asserts the number of values given on the constructor
              is equal to the number of argument Tuple values and that each
              constructor value is not <code class="code">.equals()</code> to its
              corresponding argument value.</p>
            </dd><dt><span class="term">AssertEqualsAll</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertEqualsAll</code>
              Assertion asserts that every value in the argument Tuple is
              <code class="code">.equals()</code> to the single value given on the
              constructor.</p>
            </dd><dt><span class="term">AssertExpression</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertExpression</code>
              Assertion dynamically resolves a given Java expression (see
              <a class="xref" href="#operation-expression">Expression Operations</a>) using argument Tuple
              values. Any Tuple that returns <code class="code">true</code> for the given
              expression passes the assertion.</p>
            </dd><dt><span class="term">AssertMatches</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertMatches</code>
              Assertion matches the given regular expression pattern String
              against the whole argument Tuple by joining each individual
              element of the Tuple with a TAB character (\t).</p>
            </dd><dt><span class="term">AssertMatchesAll</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertMatchesAll</code>
              Assertion matches the given regular expression pattern String
              against each argument Tuple value individually.</p>
            </dd><dt><span class="term">AssertNotNull</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertNotNull</code>
              Assertion asserts that every value in the argument Tuple is not
              a <code class="code">null</code> value.</p>
            </dd><dt><span class="term">AssertNull</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertNull</code>
              Assertion asserts that every value in the argument Tuple is a
              <code class="code">null</code> value.</p>
            </dd><dt><span class="term">AssertSizeEquals</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertSizeEquals</code>
              Assertion asserts that the current Tuple in the tuple stream is
              exactly the given size. On evaluation, <code class="code">Tuple#size()</code>
              is called (note Tuples may hold <code class="code">null</code>
              values).</p>
            </dd><dt><span class="term">AssertSizeLessThan</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertSizeLessThan</code>
              Assertion asserts that the current Tuple in the stream has a
              size less than (<code class="code">&lt;</code>) the given size. On
              evaluation, <code class="code">Tuple#size()</code> is called (note Tuples may
              hold <code class="code">null</code> values).</p>
            </dd><dt><span class="term">AssertSizeMoreThan</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertSizeMoreThan</code>
              Assertion asserts that the current Tuple in the stream has a
              size more than (<code class="code">&gt;</code>) the given size. On
              evaluation, <code class="code">Tuple#size()</code> is called (note Tuples may
              hold <code class="code">null</code> values).</p>
            </dd><dt><span class="term">AssertGroupSizeEquals</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertGroupSizeEquals</code>
              Group Assertion asserts that the number of items in the current
              grouping is equal (<code class="code">==</code>) the given size. If a pattern
              String is given, only grouping keys that match the regular
              expression will have this assertion applied where multiple key
              values are delimited by a TAB character.</p>
            </dd><dt><span class="term">AssertGroupSizeLessThan</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertGroupSizeEquals</code>
              Group Assertion asserts that the number of items in the current
              grouping is less than (<code class="code">&lt;</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</p>
            </dd><dt><span class="term">AssertGroupSizeMoreThan</span></dt><dd>
              <p>The
              <code class="classname">cascading.operation.assertion.AssertGroupSizeEquals</code>
              Group Assertion asserts that the number of items in the current
              grouping is more than (<code class="code">&gt;</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</p>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21206"></a>Logical Filter Operators</h2></div></div></div>
      

      <p>The logical <code class="classname">Filter</code> operators allows the
      user to assemble more complex filtering to be used in a single Pipe,
      instead of chaining multiple Pipes together to get the same
      effect.</p>

      <p></p><div class="variablelist"><dl><dt><span class="term">And</span></dt><dd>
              <p>The <code class="classname">cascading.operation.filter.And</code>
              <code class="classname">Filter</code> will logically 'and' the results
              of the constructor provided <code class="classname">Filter</code>
              instances. Logically, if
              <code class="methodname">Filter#isRemove()</code> returns
              <code class="code">true</code> for all given instances, this filter will
              return <code class="code">true</code>.</p>
            </dd><dt><span class="term">Or</span></dt><dd>
              <p>The <code class="classname">cascading.operation.filter.Or</code>
              <code class="classname">Filter</code> will logically 'or' the results of
              the constructor provided <code class="classname">Filter</code>
              instances. Logically, if
              <code class="methodname">Filter#isRemove()</code> returns
              <code class="code">true</code> for any of the given instances, this filter
              will return <code class="code">true</code>.</p>
            </dd><dt><span class="term">Not</span></dt><dd>
              <p>The <code class="classname">cascading.operation.filter.Not</code>
              <code class="classname">Filter</code> will logically 'not' (negation)
              the results of the constructor provided
              <code class="classname">Filter</code> instances. Logically, if
              <code class="methodname">Filter#isRemove()</code> returns
              <code class="code">true</code> for the given instance, this filter will
              return the opposite, <code class="code">false</code>.</p>
            </dd><dt><span class="term">Xor</span></dt><dd>
              <p>The <code class="classname">cascading.operation.filter.Xor</code>
              <code class="classname">Filter</code> will logically 'xor' (exclusive
              or) the results of the constructor provided
              <code class="classname">Filter</code> instances. Logically, if
              <code class="methodname">Filter#isRemove()</code> returns
              <code class="code">true</code> for all given instances, or returns
              <code class="code">false</code> for all given instances, this filter will
              return <code class="code">true</code>. Note that Xor can only be applied to
              two values.</p>
            </dd></dl></div><p></p>

      <div class="example"><a name="N21290"></a><p class="title"><b>Example&nbsp;7.1.&nbsp;Combining Filters</b></p><div class="example-contents">
        

        <pre class="programlisting">// incoming -&gt; "ip", "time", "method", "event", "status", "size"

FilterNull filterNull = new FilterNull();
RegexFilter regexFilter = new RegexFilter( "(GET|HEAD|POST)" );

And andFilter = new And( filterNull, regexFilter );

assembly = new Each( assembly, new Fields( "method" ), andFilter );

// outgoing -&gt; "ip", "time", "method", "event", "status", "size"</pre>
      </div></div><br class="example-break">

      <p>Above, we are "and-ing" the two filters. Both must be satisfied
      for the data to pass through this one Pipe.</p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N212A0"></a>Chapter&nbsp;8.&nbsp;Best Practices</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N212AA">Unit Testing</a></span></dt><dt><span class="section"><a href="#N212D7">Flow Granularity</a></span></dt><dt><span class="section"><a href="#N212EF">SubAssemblies, not Factories</a></span></dt><dt><span class="section"><a href="#N21304">Give SubAssemblies Logical Responsibilities</a></span></dt><dt><span class="section"><a href="#N21325">Java Operators in Field Names</a></span></dt><dt><span class="section"><a href="#N21337">Debugging Planner Failures</a></span></dt><dt><span class="section"><a href="#N21355">Optimizing Joins</a></span></dt><dt><span class="section"><a href="#N2135E">Debuging Streams</a></span></dt><dt><span class="section"><a href="#N2136D">Handling Good and Bad Data</a></span></dt><dt><span class="section"><a href="#N21382">Maintaining State in Operations</a></span></dt><dt><span class="section"><a href="#N213BB">Custom Types</a></span></dt><dt><span class="section"><a href="#N213C7">Fields Constants</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N212AA"></a>Unit Testing</h2></div></div></div>
      

      <p>Testing Operations, pipe-assemblies, and applications is a must.
      The <code class="classname">cascading.CascadingTestCase</code> provides a number
      of helper methods.</p>

      <p>When testing custom Operations, use the
      <code class="methodname">invokeFunction()</code>,
      <code class="methodname">invokeFilter()</code>,
      <code class="methodname">invokeAggregator()</code>, and
      <code class="methodname">invokeBuffer()</code> methods.</p>

      <p>When testing Flows, use the
      <code class="methodname">validateLength()</code> methods. There are quite a
      few, each offering extra flexibility. All of them will read the sink Tap
      and validate it is the correct length, have the correct Tuple size, and
      if the values match a given regular expression pattern.</p>

      <p>The <code class="classname">cascading.ClusterTestCase</code> can be used
      if you want to launch an embedded Hadoop cluster inside your
      TestCase.</p>

      <p>Make sure <code class="filename">cascading-test-x.y.z.jar</code> is in your
      testing class-path in order to use these helper classes.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N212D7"></a>Flow Granularity</h2></div></div></div>
      

      <p>Even though having one large <code class="classname">Flow</code> may
      result in a slightly more efficient execution plan, it is much more
      modular and flexible to give smaller Flows well defined responsibilities
      and to hand all the dependent Flow instances to a
      <code class="classname">Cascade</code> for execution as a single unit. Using the
      <code class="classname">TextDelimited</code> <code class="classname">Scheme</code>
      between <code class="classname">Flow</code> instances also provides a means to
      hand intermediate data off to other systems for reporting or QA with
      minimal penalty while remaining compatible with other tools.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N212EF"></a>SubAssemblies, not Factories</h2></div></div></div>
      

      <p>When developing your applications, use
      <code class="classname">SubAssembly</code> sub-classes, not "factory" methods.
      This way the code is much easier to read and to test.</p>

      <p>The funny thing is that <code class="classname">Object</code> constructors
      are "factories", so there isn't much reason to build frameworks to
      duplicate what a constructor already does. Of course there are
      exceptions, but in practice they are rare when you can use a
      <code class="classname">SubAssembly</code>.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21304"></a>Give SubAssemblies Logical Responsibilities</h2></div></div></div>
      

      <p>SubAssembies provide a very convenient means to co-locate like
      responsibilities into a single place. For example, have a
      <code class="classname">ParsingSubAssembly</code> and a
      <code class="classname">RulesSubAssembly</code>, where the first is responsible
      solely for parsing incoming <code class="classname">Tuple</code> streams (log
      files for example), and the second applies rules to decide if a given
      <code class="classname">Tuple</code> should be discarded or marked as
      bad.</p>

      <p>Further, in your unit tests, you can create an
      <code class="classname">TestAssertionsSubAssembly</code>, that just inlines
      various <code class="classname">ValueAssertions</code> and
      <code class="classname">GroupAssertions</code>. Inlining Assertions directly in
      your SubAssemblies is also very important, but sometimes it makes sense
      to have more tests outside of the business logic.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21325"></a>Java Operators in Field Names</h2></div></div></div>
      

      <p>There are a number of Operations in Cascading that will compile
      and apply Java expressions on the fly, see
      <code class="classname">ExpressionFunction</code> and
      <code class="classname">ExpressionFilter</code> for examples. In these
      expressions, Operation argument field names are used as variable in the
      expression. When creating field names, be conscious of the fact that if
      they are used in an expression, some characters will cause compilation
      errors. For example, "first-name" is a valid field name for use with
      Cascading, but this expression, <code class="code">first-name.trim()</code>, will
      fail.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21337"></a>Debugging Planner Failures</h2></div></div></div>
      

      <p>Oftentimes the <code class="classname">FlowConnector</code> will fail when
      attempting to plan a <code class="classname">Flow</code>. If the exception
      message given by <code class="classname">PlannerException</code> is vague, use
      the method <code class="code">PlannerException.writeDOT()</code> to export a text
      representation of the internal pipe assembly. DOT files can be opened by
      GraphViz and OmniGraffle. These plans are only partial, but you will be
      able to see where the Cascading planner failed.</p>

      <p>Also note you can create a DOT file from a
      <code class="classname">Flow</code> as well via
      <code class="code">Flow.writeDOT()</code>.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21355"></a>Optimizing Joins</h2></div></div></div>
      

      <p>When joining two streams via a CoGroup Pipe, attempt to place the
      largest of the streams in the left most argument to the CoGroup. Joining
      multiple streams requires some accumulation of values before the join
      operator can begin, but the left most stream will not be accumulated.
      This should improve the performance of most joins.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2135E"></a>Debuging Streams</h2></div></div></div>
      

      <p>When creating complex assemblies it is safe to embed
      <code class="classname">Debug</code> operations (see <a class="xref" href="#debug-function">Debug Function</a>) at appropriate debug levels where
      appropriate. Use the planner to remove them at runtime for production
      and staging runs to prevent them from using unnecessary
      resources.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2136D"></a>Handling Good and Bad Data</h2></div></div></div>
      

      <p>It is very common when processing raw data streams to encounter
      data that is corrupt or malformed in some way. This may be because bad
      content was fetched off the web via a crawler/fetcher upstream. Or a bug
      leaked into a browser widget that sends user behavior information back
      for analysis. Whatever the use-case, there is likely a set of rules that
      govern when to identify and choose to keep or discard a questionable
      record.</p>

      <p>It is tempting to simiply throw an exception and have a Trap
      capture the offending <code class="classname">Tuple</code>, but Traps were not
      designed as a filtering mechanism, and subsequently much valuable
      information would be lost.</p>

      <p>Instead create a <code class="classname">SubAssembly</code> that applies
      rules to the stream by setting a binary field that marks the tuple as
      good or bad. After all the rules are applied, split the stream based on
      the value of the good/bad boolean value. Optionally, set a reason field
      as to why the Tuple was marked bad.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21382"></a>Maintaining State in Operations</h2></div></div></div>
      

      <p>When creating custom Operations
      (<code class="classname">Function</code>,<code class="classname">
      Filter</code>,<code class="classname"> Aggregator</code>, or
      <code class="classname">Buffer</code>) do not store operation state in class
      fields. For example, if implementing a custom 'counter'
      <code class="classname">Aggregator</code>, do not create a field named 'count'
      and increment it on every
      <code class="methodname">Aggregator.aggregate()</code> call. There is no
      guarantee your Operation will be called from a single thread in a JVM,
      future version of Hadoop could execute the same operation from multiple
      threads.</p>

      <p>To maintain state across <code class="classname">Operation</code> calls,
      create and initialize a "context" object that is maintained by the
      appropriate <code class="classname">OperationCall</code>
      (<code class="classname">FilterCall</code>, <code class="classname">FunctionCall</code>,
      <code class="classname">AggregatorCall</code>, and
      <code class="classname">BufferCall</code>). In the example above, store an
      Integer 0 in the <code class="classname">AggregatorCall</code> passed to the
      <code class="methodname">Aggregator.start()</code> method and increment it in
      the <code class="methodname">Aggregator.aggregate()</code> method.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N213BB"></a>Custom Types</h2></div></div></div>
      

      <p>It is generally frowned upon to pass a custom class through a
      Tuple stream. One one hand this increases coupling of custom Operations
      to a particular type, and it removes opportunities for reducing the
      amount of data that passes over the network (or is
      serialized/deserialized).</p>

      <p>To overcome the first objection, with every custom type with
      multiple instance fields, attempt to provide Functions that can promote
      a value from the custom object to a position in a Tuple or demote the
      Tuple value to a particular field back into the custom type. This allows
      existing operations (like ExpressionFunction or RegexFilter) to operate
      on values owned by a custom type. For example, if you have a Person
      object, have a Function named GetPersonAge that takes Person as an
      argument and only returns the age as the result. The next operation can
      then Filter all Persons based on their age. This may seem like more work
      and less effiicient, but it keeps your application flexible and reduces
      the amount of duplicate code (the only alternative here is to create a
      PersonAgeFilter which results in one more thing to test).</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N213C7"></a>Fields Constants</h2></div></div></div>
      

      <p>Instead of having String field names strewn about, create an
      Interface that holds a constant value for each field name; <code class="code">public
      static Fields FIRST_NAME = new Fields( "firstname" );</code></p>

      <p>Using the Fields class instead of String allows for building more
      complex constants; <code class="code">public static Fields NAME = FIRST_NAME.append(
      LAST_NAME );</code></p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N213D8"></a>Chapter&nbsp;9.&nbsp;CookBook</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N213E5">Tuples and Fields</a></span></dt><dt><span class="section"><a href="#N2140B">Stream Shaping</a></span></dt><dt><span class="section"><a href="#N21469">Common Operations</a></span></dt><dt><span class="section"><a href="#N2148F">Stream Ordering</a></span></dt><dt><span class="section"><a href="#N214C3">API Usage</a></span></dt></dl></div>
    

    <p>Some common idioms used in Cascading applications.</p>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N213E5"></a>Tuples and Fields</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">Copy a Tuple instance</span></dt><dd>
              <pre class="programlisting">Tuple original = new Tuple( "john", "doe" );

// call copy constructor
Tuple copy = new Tuple( original );

assert copy.get( 0 ).equals( "john" );</pre>
            </dd><dt><span class="term">Nest a Tuple instance within a Tuple</span></dt><dd>
              <pre class="programlisting">Tuple parent = new Tuple();
parent.add( new Tuple( "john", "doe" ) );

assert ( (Tuple) parent.get( 0 ) ).get( 0 ).equals( "john" );</pre>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2140B"></a>Stream Shaping</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">Split (branch) a Tuple Stream</span></dt><dd>
              <pre class="programlisting">Pipe pipe = new Pipe( "head" );
pipe = new Each( pipe, new SomeFunction() );
// ...

// split left with the branch name 'lhs'
Pipe lhs = new Pipe( "lhs", pipe );
lhs = new Each( lhs, new SomeFunction() );
// ...

// split right with the branch name 'rhs'
Pipe rhs = new Pipe( "rhs", pipe );
rhs = new Each( rhs, new SomeFunction() );
// ...</pre>
            </dd><dt><span class="term">Copy a field value</span></dt><dd>
              <pre class="programlisting">Fields argument = new Fields( "field" );
Identity identity = new Identity( new Fields( "copy" ) );

// identity copies the incoming argument to the result tuple
pipe = new Each( pipe, argument, identity, Fields.ALL );</pre>
            </dd><dt><span class="term">Discard (drop) a field</span></dt><dd>
              <pre class="programlisting">// incoming -&gt; "keepField", "dropField"
pipe = new Each( pipe, new Fields( "keepField" ), new Identity(),
  Fields.RESULTS );
// outgoing -&gt; "keepField"</pre>
            </dd><dt><span class="term">Rename a field</span></dt><dd>
              <pre class="programlisting">// a simple SubAssembly that uses Identity internally
pipe = new Rename( pipe, new Fields( "from" ), new Fields( "to" ) );</pre>
            </dd><dt><span class="term">Coerce field values from Strings to primitives</span></dt><dd>
              <pre class="programlisting">Fields arguments = new Fields( "longField", "booleanField" );
Class types[] = new Class[]{long.class, boolean.class};
Identity identity = new Identity( types );

// convert from string to given type, inline replace values
pipe = new Each( pipe, arguments, identity, Fields.REPLACE );</pre>
            </dd><dt><span class="term">Insert constant values into a stream</span></dt><dd>
              <pre class="programlisting">Fields fields = new Fields( "constant1", "constant2" );
pipe = new Each( pipe, new Insert( fields, "value1", "value2" ),
  Fields.ALL );</pre>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21469"></a>Common Operations</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">Parse a String date/time value</span></dt><dd>
              <pre class="programlisting">// convert string date/time field to a long
// milliseconds "timestamp" value
String format = "yyyy:MM:dd:HH:mm:ss.SSS";
DateParser parser = new DateParser( new Fields( "ts" ), format );
pipe = new Each( pipe, new Fields( "datetime" ), parser, Fields.ALL );</pre>
            </dd><dt><span class="term">Format a time-stamp to a date/time value</span></dt><dd>
              <pre class="programlisting">// convert a long milliseconds "timestamp" value to a string
String format = "HH:mm:ss.SSS";
DateFormatter formatter = new DateFormatter( new Fields( "datetime" ),
  format );
pipe = new Each( pipe, new Fields( "ts" ), formatter, Fields.ALL );</pre>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N2148F"></a>Stream Ordering</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">Remove duplicate Tuples in a stream</span></dt><dd>
              <pre class="programlisting">// group on all values
pipe = new GroupBy( pipe, Fields.ALL );
// only take the first tuple in the grouping, ignore the rest
pipe = new Every( pipe, Fields.ALL, new First(), Fields.RESULTS );</pre>
            </dd><dt><span class="term">Create a list of unique values</span></dt><dd>
              <pre class="programlisting">// group on all unique 'ip' values
pipe = new GroupBy( pipe, new Fields( "ip" ) );
// only take one 'ip' tuple in the group
pipe = new Every( pipe, new Fields( "ip" ), new First(),
  Fields.RESULTS );</pre>
            </dd><dt><span class="term">Find first occurrence in time of a unique value</span></dt><dd>
              <pre class="programlisting">// group on all unique 'ip' values
// secondary sort on 'datetime', natural order is in ascending order
pipe = new GroupBy( pipe, new Fields( "ip" ), new Fields( "datetime" ) );
// take the first 'ip' tuple in the group which has the
// oldest 'datetime' value
pipe = new Every( pipe, Fields.ALL, new First(), Fields.RESULTS );</pre>
            </dd></dl></div><p></p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N214C3"></a>API Usage</h2></div></div></div>
      

      <p></p><div class="variablelist"><dl><dt><span class="term">Pass properties to a custom Operation</span></dt><dd>
              <pre class="programlisting">// set property on Flow
Properties properties = new Properties();
properties.put( "key", "value" );
FlowConnector flowConnector = new FlowConnector( properties );
// ...

// get the property from within an Operation (Function, Filter, etc)
String value = (String) flowProcess.getProperty( "key" );</pre>
            </dd><dt><span class="term">Bind multiple sources and sinks to a Flow</span></dt><dd>
              <pre class="programlisting">Pipe headLeft = new Pipe( "headLeft" );
// do something interesting

Pipe headRight = new Pipe( "headRight" );
// do something interesting

// merge the two input streams
Pipe merged = new GroupBy( headLeft, headRight, new Fields( "common" ) );
// ...

// branch the merged stream
Pipe tailLeft = new Pipe( "tailLeft", merged );
// filter out values to the left
tailLeft = new Each( tailLeft, new SomeFilter() );

Pipe tailRight = new Pipe( "tailRight", merged );
// filter out values to the right
tailRight = new Each( tailRight, new SomeFilter() );

// source taps
Tap sourceLeft = new Hfs( new Fields( "some-fields" ), "some/path" );
Tap sourceRight = new Hfs( new Fields( "some-fields" ), "some/path" );

Pipe[] pipesArray = Pipe.pipes( headLeft, headRight );
Tap[] tapsArray = Tap.taps( sourceLeft, sourceRight );

// a convenience function for creating branch names to tap maps
Map&lt;String, Tap&gt; sources = Cascades.tapsMap( pipesArray, tapsArray );

// sink taps
Tap sinkLeft = new Hfs( new Fields( "some-fields" ), "some/path" );
Tap sinkRight = new Hfs( new Fields( "some-fields" ), "some/path" );

pipesArray = Pipe.pipes( tailLeft, tailRight );
tapsArray = Tap.taps( sinkLeft, sinkRight );

// or create the Map manually
Map&lt;String, Tap&gt; sinks = new HashMap&lt;String,Tap&gt;();
sinks.put( tailLeft.getName(), sinkLeft );
sinks.put( tailRight.getName(), sinkRight );

// set property on Flow
FlowConnector flowConnector = new FlowConnector();

Flow flow = flowConnector.connect( "flow-name", sources, sinks, tailLeft, tailRight );</pre>
            </dd></dl></div><p></p>
    </div>
  </div>

  <div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="N214EA"></a>Chapter&nbsp;10.&nbsp;How It Works</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#N214F4">MapReduce Job Planner</a></span></dt><dt><span class="section"><a href="#N21530">The Cascade Topological Scheduler</a></span></dt></dl></div>
    

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N214F4"></a>MapReduce Job Planner</h2></div></div></div>
      

      <p>The MapReduce Job Planner is an internal feature of
      Cascading.</p>

      <p>When a collection of functions, splits, and joins are all tied up
      together into a 'pipe assembly', the FlowConnector object is used to
      create a new Flow instance against input and output data paths. This
      Flow is a single Cascading job.</p>

      <p>Internally the FlowConnector employs an intelligent planner to
      convert the pipe assembly to a graph of dependent MapReduce jobs that
      can be executed on a Hadoop cluster.</p>

      <p>All this happens under the scenes. As is the scheduling of the
      individual MapReduce jobs, and the clean up of intermediate data sets
      that bind the jobs together.</p>

      <p><span class="inlinemediaobject"><img src="Cascading%20-%20User%20Guide_files/planned-flow.png" alt="A Flow partitioned by MapReduce tasks" align="middle" width="630"></span></p>

      <p>Above we can see how a reasonably normal Flow would be partitioned
      into MapReduce jobs. Every job is delimited by a temporary file that is
      the sink from the first job, and then the source to the next job.</p>

      <p>To see how your Flows are partitioned, call the
      <code class="classname">Flow#writeDOT()</code> method. This will write a <a class="link" href="http://en.wikipedia.org/wiki/DOT_language" target="_top">DOT</a> file
      out to the path specified, and can be imported into a graphics package
      like OmniGraffle or Graphviz.</p>
    </div>

    <div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both;"><a name="N21530"></a>The Cascade Topological Scheduler</h2></div></div></div>
      

      <p>Cascading has a simple class, <code class="classname">Cascade</code> ,
      that will take a collection of Cascading Flows and execute them on the
      target cluster in dependency order.</p>

      <p>Consider the following example.</p>

      <div class="itemizedlist"><ul type="disc"><li>
          <p>Flow 'first' reads input file A and outputs B.</p>
        </li><li>
          <p>Flow 'second' expects input B and outputs C and D.</p>
        </li><li>
          <p>Flow 'third' expects input C and outputs E.</p>
        </li></ul></div>

      <p>A <code class="classname">Cascade</code> is constructed through the
      <code class="classname">CascadeConnector</code> class, by building an internal
      graph that makes each Flow a 'vertex', and each file an 'edge'. A
      topological walk on this graph will touch each vertex in order of its
      dependencies. When a vertex has all it's incoming edges (files)
      available, it will be scheduled on the cluster.</p>

      <p>In the example above, 'first' goes first, 'second' goes second,
      and 'third' is last.</p>

      <p>If two or more Flows are independent of one another, they will be
      scheduled concurrently.</p>

      <p>And by default, if any outputs from a Flow are newer than the
      inputs, the Flow is skipped. The assumption is that the Flow was
      executed recently, since the output isn't stale. So there is no reason
      to re-execute it and use up resources or add time to the job. This is
      similar behaviour a compiler would exhibit if a source file wasn't
      updated before a recompile.</p>

      <p>This is very handy if you have a large number of jobs that should
      be executed as a logical unit with varying dependencies between them.
      Just pass them to the CascadeConnector, and let it sort them all
      out.</p>
    </div>
  </div>
</div></body></html>